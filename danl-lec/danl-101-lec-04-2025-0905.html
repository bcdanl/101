<!DOCTYPE html>
<html lang="en"><head>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-html/tabby.min.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/light-border.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-html.min.css" rel="stylesheet" data-mode="light">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles"><meta charset="utf-8">
  <meta name="generator" content="quarto-1.3.450">

  <meta name="author" content="Byeong-Hak Choe">
  <meta name="dcterms.date" content="2025-09-05">
  <title>Lecture 4</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="../site_libs/revealjs/dist/reset.css">
  <link rel="stylesheet" href="../site_libs/revealjs/dist/reveal.css">
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      width: 0.8em;
      margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
      vertical-align: middle;
    }
  </style>
  <link rel="stylesheet" href="../site_libs/revealjs/dist/theme/quarto.css">
  <link href="../site_libs/revealjs/plugin/quarto-line-highlight/line-highlight.css" rel="stylesheet">
  <link href="../site_libs/revealjs/plugin/reveal-menu/menu.css" rel="stylesheet">
  <link href="../site_libs/revealjs/plugin/reveal-menu/quarto-menu.css" rel="stylesheet">
  <link href="../site_libs/revealjs/plugin/reveal-chalkboard/font-awesome/css/all.css" rel="stylesheet">
  <link href="../site_libs/revealjs/plugin/reveal-chalkboard/style.css" rel="stylesheet">
  <link href="../site_libs/revealjs/plugin/quarto-support/footer.css" rel="stylesheet">
  <style type="text/css">

  .callout {
    margin-top: 1em;
    margin-bottom: 1em;  
    border-radius: .25rem;
  }

  .callout.callout-style-simple { 
    padding: 0em 0.5em;
    border-left: solid #acacac .3rem;
    border-right: solid 1px silver;
    border-top: solid 1px silver;
    border-bottom: solid 1px silver;
    display: flex;
  }

  .callout.callout-style-default {
    border-left: solid #acacac .3rem;
    border-right: solid 1px silver;
    border-top: solid 1px silver;
    border-bottom: solid 1px silver;
  }

  .callout .callout-body-container {
    flex-grow: 1;
  }

  .callout.callout-style-simple .callout-body {
    font-size: 1rem;
    font-weight: 400;
  }

  .callout.callout-style-default .callout-body {
    font-size: 0.9rem;
    font-weight: 400;
  }

  .callout.callout-titled.callout-style-simple .callout-body {
    margin-top: 0.2em;
  }

  .callout:not(.callout-titled) .callout-body {
      display: flex;
  }

  .callout:not(.no-icon).callout-titled.callout-style-simple .callout-content {
    padding-left: 1.6em;
  }

  .callout.callout-titled .callout-header {
    padding-top: 0.2em;
    margin-bottom: -0.2em;
  }

  .callout.callout-titled .callout-title  p {
    margin-top: 0.5em;
    margin-bottom: 0.5em;
  }
    
  .callout.callout-titled.callout-style-simple .callout-content  p {
    margin-top: 0;
  }

  .callout.callout-titled.callout-style-default .callout-content  p {
    margin-top: 0.7em;
  }

  .callout.callout-style-simple div.callout-title {
    border-bottom: none;
    font-size: .9rem;
    font-weight: 600;
    opacity: 75%;
  }

  .callout.callout-style-default  div.callout-title {
    border-bottom: none;
    font-weight: 600;
    opacity: 85%;
    font-size: 0.9rem;
    padding-left: 0.5em;
    padding-right: 0.5em;
  }

  .callout.callout-style-default div.callout-content {
    padding-left: 0.5em;
    padding-right: 0.5em;
  }

  .callout.callout-style-simple .callout-icon::before {
    height: 1rem;
    width: 1rem;
    display: inline-block;
    content: "";
    background-repeat: no-repeat;
    background-size: 1rem 1rem;
  }

  .callout.callout-style-default .callout-icon::before {
    height: 0.9rem;
    width: 0.9rem;
    display: inline-block;
    content: "";
    background-repeat: no-repeat;
    background-size: 0.9rem 0.9rem;
  }

  .callout-title {
    display: flex
  }
    
  .callout-icon::before {
    margin-top: 1rem;
    padding-right: .5rem;
  }

  .callout.no-icon::before {
    display: none !important;
  }

  .callout.callout-titled .callout-body > .callout-content > :last-child {
    margin-bottom: 0.5rem;
  }

  .callout.callout-titled .callout-icon::before {
    margin-top: .5rem;
    padding-right: .5rem;
  }

  .callout:not(.callout-titled) .callout-icon::before {
    margin-top: 1rem;
    padding-right: .5rem;
  }

  /* Callout Types */

  div.callout-note {
    border-left-color: #4582ec !important;
  }

  div.callout-note .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAAEU0lEQVRYCcVXTWhcVRQ+586kSUMMxkyaElstCto2SIhitS5Ek8xUKV2poatCcVHtUlFQk8mbaaziwpWgglJwVaquitBOfhQXFlqlzSJpFSpIYyXNjBNiTCck7x2/8/LeNDOZxDuEkgOXe++553zfefee+/OYLOXFk3+1LLrRdiO81yNqZ6K9cG0P3MeFaMIQjXssE8Z1JzLO9ls20MBZX7oG8w9GxB0goaPrW5aNMp1yOZIa7Wv6o2ykpLtmAPs/vrG14Z+6d4jpbSKuhdcSyq9wGMPXjonwmESXrriLzFGOdDBLB8Y6MNYBu0dRokSygMA/mrun8MGFN3behm6VVAwg4WR3i6FvYK1T7MHo9BK7ydH+1uurECoouk5MPRyVSBrBHMYwVobG2aOXM07sWrn5qgB60rc6mcwIDJtQrnrEr44kmy+UO9r0u9O5/YbkS9juQckLed3DyW2XV/qWBBB3ptvI8EUY3I9p/67OW+g967TNr3Sotn3IuVlfMLVnsBwH4fsnebJvyGm5GeIUA3jljERmrv49SizPYuq+z7c2H/jlGC+Ghhupn/hcapqmcudB9jwJ/3jvnvu6vu5lVzF1fXyZuZZ7U8nRmVzytvT+H3kilYvH09mLWrQdwFSsFEsxFVs5fK7A0g8gMZjbif4ACpKbjv7gNGaD8bUrlk8x+KRflttr22JEMRUbTUwwDQScyzPgedQHZT0xnx7ujw2jfVfExwYHwOsDTjLdJ2ebmeQIlJ7neo41s/DrsL3kl+W2lWvAga0tR3zueGr6GL78M3ifH0rGXrBC2aAR8uYcIA5gwV8zIE8onoh8u0Fca/ciF7j1uOzEnqcIm59sEXoGc0+z6+H45V1CvAvHcD7THztu669cnp+L0okAeIc6zjbM/24LgGM1gZk7jnRu1aQWoU9sfUOuhrmtaPIO3YY1KLLWZaEO5TKUbMY5zx8W9UJ6elpLwKXbsaZ4EFl7B4bMtDv0iRipKoDQT2sNQI9b1utXFdYisi+wzZ/ri/1m7QfDgEuvgUUEIJPq3DhX/5DWNqIXDOweC2wvIR90Oq3lDpdMIgD2r0dXvGdsEW5H6x6HLRJYU7C69VefO1x8Gde1ZFSJLfWS1jbCnhtOPxmpfv2LXOA2Xk2tvnwKKPFuZ/oRmwBwqRQDcKNeVQkYcOjtWVBuM/JuYw5b6isojIkYxyYAFn5K7ZBF10fea52y8QltAg6jnMqNHFBmGkQ1j+U43HMi2xMar1Nv0zGsf1s8nUsmUtPOOrbFIR8bHFDMB5zL13Gmr/kGlCkUzedTzzmzsaJXhYawnA3UmARpiYj5ooJZiUoxFRtK3X6pgNPv+IZVPcnwbOl6f+aBaO1CNvPW9n9LmCp01nuSaTRF2YxHqZ8DYQT6WsXT+RD6eUztwYLZ8rM+rcPxamv1VQzFUkzFXvkiVrySGQgJNvXHJAxiU3/NwiC03rSf05VBaPtu/Z7/B8Yn/w7eguloAAAAAElFTkSuQmCC');
  }

  div.callout-note.callout-style-default .callout-title {
    background-color: #dae6fb
  }

  div.callout-important {
    border-left-color: #d9534f !important;
  }

  div.callout-important .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAAEKklEQVRYCcVXTWhcVRS+575MJym48A+hSRFr00ySRQhURRfd2HYjk2SSTokuBCkU2o0LoSKKraKIBTcuFCoidGFD08nkBzdREbpQ1EDNIv8qSGMFUboImMSZd4/f9zJv8ibJMC8xJQfO3HPPPef7zrvvvnvviIkpC9nsw0UttFunbUhpFzFtarSd6WJkStVMw5xyVqYTvkwfzuf/5FgtkVoB0729j1rjXwThS7Vio+Mo6DNnvLfahoZ+i/o32lULuJ3NNiz7q6+pyAUkJaFF6JwaM2lUJlV0MlnQn5aTRbEu0SEqHUa0A4AdiGuB1kFXRfVyg5d87+Dg4DL6m2TLAub60ilj7A1Ec4odSAc8X95sHh7+ZRPCFo6Fnp7HfU/fBng/hi10CjCnWnJjsxvDNxWw0NfV6Rv5GgP3I3jGWXumdTD/3cbEOP2ZbOZp69yniG3FQ9z1jD7bnBu9Fc2tKGC2q+uAJOQHBDRiZX1x36o7fWBs7J9ownbtO+n0/qWkvW7UPIfc37WgT6ZGR++EOJyeQDSb9UB+DZ1G6DdLDzyS+b/kBCYGsYgJbSQHuThGKRcw5xdeQf8YdNHsc6ePXrlSYMBuSIAFTGAtQo+VuALo4BX83N190NWZWbynBjhOHsmNfFWLeL6v+ynsA58zDvvAC8j5PkbOcXCMg2PZFk3q8MjI7WAG/Dp9AwP7jdGBOOQkAvlFUB+irtm16I1Zw9YBcpGTGXYmk3kQIC/Cds55l+iMI3jqhjAuaoe+am2Jw5GT3Nbz3CkE12NavmzN5+erJW7046n/CH1RO/RVa8lBLozXk9uqykkGAyRXLWlLv5jyp4RFsG5vGVzpDLnIjTWgnRy2Rr+tDKvRc7Y8AyZq10jj8DqXdnIRNtFZb+t/ZRtXcDiVnzpqx8mPcDWxgARUqx0W1QB9MeUZiNrV4qP+Ehc+BpNgATsTX8ozYKL2NtFYAHc84fG7ndxUPr+AR/iQSns7uSUufAymwDOb2+NjK27lEFocm/EE2WpyIy/Hi66MWuMKJn8RvxIcj87IM5Vh9663ziW36kR0HNenXuxmfaD8JC7tfKbrhFr7LiZCrMjrzTeGx+PmkosrkNzW94ObzwocJ7A1HokLolY+AvkTiD/q1H0cN48c5EL8Crkttsa/AXQVDmutfyku0E7jShx49XqV3MFK8IryDhYVbj7Sj2P2eBxwcXoe8T8idsKKPRcnZw1b+slFTubwUwhktrfnAt7J++jwQtLZcm3sr9LQrjRzz6cfMv9aLvgmnAGvpoaGLxM4mAEaLV7iAzQ3oU0IvD5x9ix3yF2RAAuYAOO2f7PEFWCXZ4C9Pb2UsgDeVnFSpbFK7/IWu7TPTvBqzbGdCHOJQSxiEjt6IyZmxQyEJHv6xyQsYk//moVFsN2zP6fRImjfq7/n/wFDguUQFNEwugAAAABJRU5ErkJggg==');
  }

  div.callout-important.callout-style-default .callout-title {
    background-color: #f7dddc
  }

  div.callout-warning {
    border-left-color: #f0ad4e !important;
  }

  div.callout-warning .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAAETklEQVRYCeVWW2gcVRg+58yaTUnizqbipZeX4uWhBEniBaoUX1Ioze52t7sRq6APio9V9MEaoWlVsFasRq0gltaAPuxms8lu0gcviE/FFOstVbSIxgcv6SU7EZqmdc7v9+9mJtNks51NTUH84ed889/PP+cmxP+d5FIbMJmNbpREu4WUkiTtCicKny0l1pIKmBzovF2S+hIJHX8iEu3hZJ5lNZGqyRrGSIQpq15AzF28jgpeY6yk6GVdrfFqdrD6Iw+QlB8g0YS2g7dyQmXM/IDhBhT0UCiRf59lfqmmDvzRt6kByV/m4JjtzuaujMUM2c5Z2d6JdKrRb3K2q6mA+oYVz8JnDdKPmmNthzkAk/lN63sYPgevrguc72aZX/L9C6x09GYyxBgCX4NlvyGUHOKELlm5rXeR1kchuChJt4SSwyddZRXgvwMGvYo4QSlk3/zkHD8UHxwVJA6zjZZqP8v8kK8OWLnIZtLyCAJagYC4rTGW/9Pqj92N/c+LUaAj27movwbi19tk/whRCIE7Q9vyI6yvRpftAKVTdUjOW40X3h5OXsKCdmFcx0xlLJoSuQngnrJe7Kcjm4OMq9FlC7CMmScQANuNvjfP3PjGXDBaUQmbp296S5L4DrpbrHN1T87ZVEZVCzg1FF0Ft+dKrlLukI+/c9ENo+TvlTDbYFvuKPtQ9+l052rXrgKoWkDAFnvh0wTOmYn8R5f4k/jN/fZiCM1tQx9jQQ4ANhqG4hiL0qIFTGViG9DKB7GYzgubnpofgYRwO+DFjh0Zin2m4b/97EDkXkc+f6xYAPX0KK2I/7fUQuwzuwo/L3AkcjugPNixC8cHf0FyPjWlItmLxWw4Ou9YsQCr5fijMGoD/zpdRy95HRysyXA74MWOnscpO4j2y3HAVisw85hX5+AFBRSHt4ShfLFkIMXTqyKFc46xdzQM6XbAi702a7sy04J0+feReMFKp5q9esYLCqAZYw/k14E/xcLLsFElaornTuJB0svMuJINy8xkIYuL+xPAlWRceH6+HX7THJ0djLUom46zREu7tTkxwmf/FdOZ/sh6Q8qvEAiHpm4PJ4a/doJe0gH1t+aHRgCzOvBvJedEK5OFE5jpm4AGP2a8Dxe3gGJ/pAutug9Gp6he92CsSsWBaEcxGx0FHytmIpuqGkOpldqNYQK8cSoXvd+xLxXADw0kf6UkJNFtdo5MOgaLjiQOQHcn+A6h5NuL2s0qsC2LOM75PcF3yr5STuBSAcGG+meA14K/CI21HcS4LBT6tv0QAh8Dr5l93AhZzG5ZJ4VxAqdZUEl9z7WJ4aN+svMvwHHL21UKTd1mqvChH7/Za5xzXBBKrUcB0TQ+Ulgkfbi/H/YT5EptrGzsEK7tR1B7ln9BBwckYfMiuSqklSznIuoIIOM42MQO+QnduCoFCI0bpkzjCjddHPN/F+2Yu+sd9bKNpVwHhbS3LluK/0zgfwD0xYI5dXuzlQAAAABJRU5ErkJggg==');
  }

  div.callout-warning.callout-style-default .callout-title {
    background-color: #fcefdc
  }

  div.callout-tip {
    border-left-color: #02b875 !important;
  }

  div.callout-tip .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAADr0lEQVRYCe1XTWgTQRj9ZjZV8a9SPIkKgj8I1bMHsUWrqYLVg4Ue6v9BwZOxSYsIerFao7UiUryIqJcqgtpimhbBXoSCVxUFe9CTiogUrUp2Pt+3aUI2u5vdNh4dmMzOzHvvezuz8xNFM0mjnbXaNu1MvFWRXkXEyE6aYOYJpdW4IXuA4r0fo8qqSMDBU0v1HJUgVieAXxzCsdE/YJTdFcVIZQNMyhruOMJKXYFoLfIfIvVIMWdsrd+Rpd86ZmyzzjJmLStqRn0v8lzkb4rVIXvnpScOJuAn2ACC65FkPzEdEy4TPWRLJ2h7z4cArXzzaOdKlbOvKKX25Wl00jSnrwVxAg3o4dRxhO13RBSdNvH0xSARv3adTXbBdTf64IWO2vH0LT+cv4GR1DJt+DUItaQogeBX/chhbTBxEiZ6gftlDNXTrvT7co4ub5A6gp9HIcHvzTa46OS5fBeP87Qm0fQkr4FsYgVQ7Qg+ZayaDg9jhg1GkWj8RG6lkeSacrrHgDaxdoBiZPg+NXV/KifMuB6//JmYH4CntVEHy/keA6x4h4CU5oFy8GzrBS18cLJMXcljAKB6INjWsRcuZBWVaS3GDrqB7rdapVIeA+isQ57Eev9eCqzqOa81CY05VLd6SamW2wA2H3SiTbnbSxmzfp7WtKZkqy4mdyAlGx7ennghYf8voqp9cLSgKdqNfa6RdRsAAkPwRuJZNbpByn+RrJi1RXTwdi8RQF6ymDwGMAtZ6TVE+4uoKh+MYkcLsT0Hk8eAienbiGdjJHZTpmNjlbFJNKDVAp2fJlYju6IreQxQ08UJDNYdoLSl6AadO+fFuCQqVMB1NJwPm69T04Wv5WhfcWyfXQB+wXRs1pt+nCknRa0LVzSA/2B+a9+zQJadb7IyyV24YAxKp2Jqs3emZTuNnKxsah+uabKbMk7CbTgJx/zIgQYErIeTKRQ9yD9wxVof5YolPHqaWo7TD6tJlh7jQnK5z2n3+fGdggIOx2kaa2YI9QWarc5Ce1ipNWMKeSG4DysFF52KBmTNMmn5HqCFkwy34rDg05gDwgH3bBi+sgFhN/e8QvRn8kbamCOhgrZ9GJhFDgfcMHzFb6BAtjKpFhzTjwv1KCVuxHvCbsSiEz4CANnj84cwHdFXAbAOJ4LTSAawGWFn5tDhLMYz6nWeU2wJfIhmIJBefcd/A5FWQWGgrWzyORZ3Q6HuV+Jf0Bj+BTX69fm1zWgK7By1YTXchFDORywnfQ7GpzOo6S+qECrsx2ifVQAAAABJRU5ErkJggg==');
  }

  div.callout-tip.callout-style-default .callout-title {
    background-color: #ccf1e3
  }

  div.callout-caution {
    border-left-color: #fd7e14 !important;
  }

  div.callout-caution .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAACV0lEQVRYCdVWzWoUQRCuqp2ICBLJXgITZL1EfQDBW/bkzUMUD7klD+ATSHBEfAIfQO+iXsWDxJsHL96EHAwhgzlkg8nBg25XWb0zIb0zs9muYYWkoKeru+vn664fBqElyZNuyh167NXJ8Ut8McjbmEraKHkd7uAnAFku+VWdb3reSmRV8PKSLfZ0Gjn3a6Xlcq9YGb6tADjn+lUfTXtVmaZ1KwBIvFI11rRXlWlatwIAAv2asaa9mlB9wwygiDX26qaw1yYPzFXg2N1GgG0FMF8Oj+VIx7E/03lHx8UhvYyNZLN7BwSPgekXXLribw7w5/c8EF+DBK5idvDVYtEEwMeYefjjLAdEyQ3M9nfOkgnPTEkYU+sxMq0BxNR6jExrAI31H1rzvLEfRIdgcv1XEdj6QTQAS2wtstEALLG1yEZ3QhH6oDX7ExBSFEkFINXH98NTrme5IOaaA7kIfiu2L8A3qhH9zRbukdCqdsA98TdElyeMe5BI8Rs2xHRIsoTSSVFfCFCWGPn9XHb4cdobRIWABNf0add9jakDjQJpJ1bTXOJXnnRXHRf+dNL1ZV1MBRCXhMbaHqGI1JkKIL7+i8uffuP6wVQAzO7+qVEbF6NbS0LJureYcWXUUhH66nLR5rYmva+2tjRFtojkM2aD76HEGAD3tPtKM309FJg5j/K682ywcWJ3PASCcycH/22u+Bh7Aa0ehM2Fu4z0SAE81HF9RkB21c5bEn4Dzw+/qNOyXr3DCTQDMBOdhi4nAgiFDGCinIa2owCEChUwD8qzd03PG+qdW/4fDzjUMcE1ZpIAAAAASUVORK5CYII=');
  }

  div.callout-caution.callout-style-default .callout-title {
    background-color: #ffe5d0
  }

  </style>
  <style type="text/css">
    .reveal div.sourceCode {
      margin: 0;
      overflow: auto;
    }
    .reveal div.hanging-indent {
      margin-left: 1em;
      text-indent: -1em;
    }
    .reveal .slide:not(.center) {
      height: 100%;
    }
    .reveal .slide.scrollable {
      overflow-y: auto;
    }
    .reveal .footnotes {
      height: 100%;
      overflow-y: auto;
    }
    .reveal .slide .absolute {
      position: absolute;
      display: block;
    }
    .reveal .footnotes ol {
      counter-reset: ol;
      list-style-type: none; 
      margin-left: 0;
    }
    .reveal .footnotes ol li:before {
      counter-increment: ol;
      content: counter(ol) ". "; 
    }
    .reveal .footnotes ol li > p:first-child {
      display: inline-block;
    }
    .reveal .slide ul,
    .reveal .slide ol {
      margin-bottom: 0.5em;
    }
    .reveal .slide ul li,
    .reveal .slide ol li {
      margin-top: 0.4em;
      margin-bottom: 0.2em;
    }
    .reveal .slide ul[role="tablist"] li {
      margin-bottom: 0;
    }
    .reveal .slide ul li > *:first-child,
    .reveal .slide ol li > *:first-child {
      margin-block-start: 0;
    }
    .reveal .slide ul li > *:last-child,
    .reveal .slide ol li > *:last-child {
      margin-block-end: 0;
    }
    .reveal .slide .columns:nth-child(3) {
      margin-block-start: 0.8em;
    }
    .reveal blockquote {
      box-shadow: none;
    }
    .reveal .tippy-content>* {
      margin-top: 0.2em;
      margin-bottom: 0.7em;
    }
    .reveal .tippy-content>*:last-child {
      margin-bottom: 0.2em;
    }
    .reveal .slide > img.stretch.quarto-figure-center,
    .reveal .slide > img.r-stretch.quarto-figure-center {
      display: block;
      margin-left: auto;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-left,
    .reveal .slide > img.r-stretch.quarto-figure-left  {
      display: block;
      margin-left: 0;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-right,
    .reveal .slide > img.r-stretch.quarto-figure-right  {
      display: block;
      margin-left: auto;
      margin-right: 0; 
    }
  </style>
</head>
<body class="quarto-light">
  <div class="reveal">
    <div class="slides">

<section id="title-slide" class="quarto-title-block center">
  <h1 class="title">Lecture 4</h1>
  <p class="subtitle">Generative AI</p>

<div class="quarto-title-authors">
<div class="quarto-title-author">
<div class="quarto-title-author-name">
Byeong-Hak Choe 
</div>
<div class="quarto-title-author-email">
<a href="mailto:bchoe@geneseo.edu">bchoe@geneseo.edu</a>
</div>
        <p class="quarto-title-affiliation">
            SUNY Geneseo
          </p>
    </div>
</div>

  <p class="date">September 5, 2025</p>
</section>
<section>
<section id="the-concepts-of-ai" class="title-slide slide level1 center" data-background-color="#1c4982">
<h1><strong>The Concepts of AI</strong></h1>

</section>
<section id="what-is-ai" class="slide level2">
<h2>What is <strong>AI</strong>?</h2>
<div class="columns">
<div class="column" style="width:40%;">
<div style="text-align: center; width: 100%; margin: auto;">
<img src="https://bcdanl.github.io/lec_figs/ai-ml-dl-difference.jpeg" style="width: 100%; margin-bottom: -20px;">
<p style="font-weight: bold;">
</p>
</div>
</div><div class="column" style="width:60%;">
<ul>
<li class="fragment"><strong>Artificial Intelligence (AI):</strong> Techniques that enable machines to perform tasks associated with human intelligence (perception, reasoning, learning, generation, action).</li>
</ul>
</div>
</div>
<ul>
<li class="fragment">In practice today: <strong>machine learning</strong> algorithms trained on data to make predictions or generate outputs.</li>
<li class="fragment">Sub‚Äëareas: machine learning; deep learning; <strong>generative</strong> models.</li>
</ul>
<!-- ## What is an **Deep Learning** and **Neural Network**? -->
<!-- ::::{.columns} -->
<!-- ::: {.column width="37.5%"} -->
<!-- <div style="text-align: center; width: 75%; margin: auto;"> -->
<!--   <img src="https://bcdanl.github.io/lec_figs/nobel-2024-physics.jpeg" style="width: 100%; margin-bottom: -20px;"> -->
<!--   <p style="font-weight: bold;"></p> -->
<!-- </div> -->
<!-- ::: -->
<!-- ::: {.column width="62.5%"} -->
<!-- - **Deep learning** is an advanced ML methodology. All deep learning is ML. -->
<!--   - It combines statistics and mathematics with **neural network** architecture. -->
<!-- ::: -->
<!-- :::: -->
<!-- - A **neural network** is a method in AI that teaches computers to process data in a way that is inspired by the human brain.  -->
<!--   - It uses interconnected nodes or neurons in a layered structure that resembles the human brain. -->
<!--   - Each connection between neurons has an associated **weight**, which determines the strength and importance of the input in influencing the output. Through training, these weights are adjusted to improve accuracy.   -->
<!-- - **Deep learning** is best for complex tasks that make sense of unstructured data, such as images, texts, and sounds. -->
<!--   - e.g., Image recognition -->
</section>
<section id="what-is-deep-learning" class="slide level2">
<h2>What is <strong>Deep Learning</strong>?</h2>
<div class="columns">
<div class="column" style="width:37.5%;">
<div style="text-align: center; width: 75%; margin: auto;">
<img src="https://bcdanl.github.io/lec_figs/nobel-2024-physics.jpeg" style="width: 100%; margin-bottom: -20px;">
<p style="font-weight: bold;">
</p>
</div>
</div><div class="column" style="width:62.5%;">
<ul>
<li class="fragment"><strong>Deep learning</strong> is an advanced machine learning methodology.
<ul>
<li class="fragment">All deep learning is machine learning, but not all ML is deep learning.<br>
</li>
<li class="fragment">It combines <strong>statistics</strong>, <strong>mathematics</strong>, and <strong>neural network</strong> architecture.</li>
</ul></li>
</ul>
</div>
</div>
<ul>
<li class="fragment"><strong>Deep learning</strong> is particularly suited for <strong>complex tasks</strong> that involve unstructured data, such as:
<ul>
<li class="fragment">Images üñºÔ∏è<br>
</li>
<li class="fragment">Texts üìù<br>
</li>
<li class="fragment">Sounds üéµ<br>
</li>
<li class="fragment">e.g., <strong>Image recognition</strong></li>
</ul></li>
</ul>
</section>
<section id="what-is-a-neural-network" class="slide level2">
<h2>What is a <strong>Neural Network</strong>?</h2>
<div style="text-align: center; width: 50%; margin: auto;">
<img src="https://bcdanl.github.io/lec_figs/neural-network-cat.jpg" style="width: 100%; margin-bottom: -20px;">
<p style="font-weight: bold;">
</p>
</div>
<ul>
<li class="fragment"><p>A <strong>neural network</strong> is a method in AI inspired by the way the human brain processes information.</p></li>
<li class="fragment"><p>It uses <strong>interconnected nodes (neurons)</strong> arranged in layers:</p>
<ul>
<li class="fragment"><strong>Input layer</strong> ‚Üí receives data<br>
</li>
<li class="fragment"><strong>Hidden layers</strong> ‚Üí transform data through computations<br>
</li>
<li class="fragment"><strong>Output layer</strong> ‚Üí produces the result</li>
</ul></li>
</ul>
</section>
<section id="what-is-a-weight-in-a-neural-network" class="slide level2">
<h2>What is a <strong>Weight</strong> in a <strong>Neural Network</strong>?</h2>
<ul>
<li class="fragment">Each connection between neurons carries a <strong>weight</strong>:
<ul>
<li class="fragment">Determines the strength and importance of the input.<br>
</li>
<li class="fragment">During <strong>training</strong>, these weights are adjusted to improve predictions.
<ul>
<li class="fragment">With multiple layers, networks capture <strong>intricate connections</strong> and represent <strong>complex patterns</strong> in data.</li>
</ul></li>
</ul></li>
</ul>
</section>
<section id="what-is-a-token" class="slide level2">
<h2>What is a Token?</h2>
<ul>
<li class="fragment">A <strong>token</strong> is the smallest unit of text an LLM processes.
<ul>
<li class="fragment"><strong>Input &amp; Output</strong> are measured in tokens, not words.</li>
</ul></li>
<li class="fragment">It can be:
<ul>
<li class="fragment">A <strong>single character</strong> (<code>a</code>, <code>!</code>)<br>
</li>
<li class="fragment">A <strong>whole word</strong> (<code>dog</code>, <code>house</code>)<br>
</li>
<li class="fragment">A <strong>part of a word</strong> (<code>play</code> + <code>ing</code>)<br>
</li>
</ul></li>
<li class="fragment">Examples of Tokenization
<ul>
<li class="fragment"><code>"cat"</code> ‚Üí <strong>1 token</strong><br>
</li>
<li class="fragment"><code>"playing"</code> ‚Üí <strong>2 tokens</strong> (<code>play</code>, <code>ing</code>)<br>
</li>
<li class="fragment"><code>"extraordinary"</code> ‚Üí <strong>2 tokens</strong> (<code>extra</code>, <code>ordinary</code>)</li>
</ul></li>
</ul>
</section>
<section id="what-is-an-llm" class="slide level2">
<h2>What is an <strong>LLM</strong>?</h2>
<ul>
<li class="fragment"><p><strong>Large Language Model (LLM):</strong> A <strong>neural network</strong> trained on vast text (and often code) to model the <strong>probability of the next token</strong>.</p></li>
<li class="fragment"><p><strong>Capabilities emerge</strong>: dialogue, summarization, code generation, reasoning heuristics, tool use (e.g., ChatGPT, Claude, Gemini, Copilot, Grok).</p></li>
<li class="fragment"><p>Limitations: <strong>hallucinations</strong> (confidently wrong), <strong>training bias</strong>, <strong>context limits</strong> (a fixed number of tokens), <strong>lack of grounding</strong>.</p></li>
</ul>
<!-- ##  What is **AGI**? -->
<!-- - **Artificial General Intelligence (AGI):** A hypothetical system that can **perform at or above human level across most cognitive tasks**. -->
<!-- - Not a settled definition; **timelines and feasibility are debated**. -->
<!-- - For our course: focus on **practical augmentation** now; track, but do not depend on, AGI speculation. -->
<!-- ## Why Tokens Matter? -->
<!-- - **Input & Output** are measured in tokens, not words.   -->
<!-- - **Costs & Limits**: LLMs have token limits (e.g., 8k, 32k, 128k).   -->
<!--   - Both **your prompt** and **the model‚Äôs reply** count.   -->
<!-- - **Training**: LLMs learn relationships between tokens, not whole words.  -->
</section></section>
<section>
<section id="introduction-living-and-working-with-ai" class="title-slide slide level1 center" data-background-color="#1c4982">
<h1><strong>Introduction: Living and Working with AI</strong></h1>

</section>
<section id="the-three-sleepless-nights" class="slide level2">
<h2>The ‚ÄúThree Sleepless Nights‚Äù</h2>
<div class="columns">
<div class="column" style="width:50%;">
<div style="text-align: center; width: 100%; margin: auto;">
<img src="https://bcdanl.github.io/lec_figs/ethan-mollick-book.png" style="width: 100%; margin-bottom: -20px;">
<p style="font-weight: bold;">
</p>
</div>
</div><div class="column" style="width:50%;">
<ul>
<li class="fragment">After hands‚Äëon use, many realize <strong>LLMs don‚Äôt behave like normal software</strong>; they feel conversational, improvisational, even <em>social</em>.</li>
<li class="fragment">This triggers excitement and anxiety: <em>What will my job be like?</em> <em>What careers remain?</em> <em>Is the model ‚Äúthinking‚Äù?</em></li>
</ul>
</div>
</div>
<ul>
<li class="fragment">The author describes staying up late <strong>trying ‚Äúimpossible‚Äù prompts</strong>‚Äîand seeing plausible solutions.</li>
<li class="fragment">Key takeaway: <strong>Perceived capability jump</strong> ‚Üí a sense that <strong>the world has changed</strong>.</li>
</ul>
</section>
<section id="a-classroom-turning-point" class="slide level2">
<h2>A Classroom Turning Point</h2>
<ul>
<li class="fragment">In late 2022, a demo for undergrads showed AI as <strong>cofounder</strong>: brainstorming ideas, drafting business plans, even playful transforms (e.g., poetry).</li>
<li class="fragment">Students rapidly <strong>built working demos</strong> using unfamiliar libraries‚Äîwith AI guidance‚Äî<strong>faster</strong> than before.</li>
<li class="fragment">Immediate classroom effects:
<ul>
<li class="fragment">Fewer raised hands (ask AI later); <strong>polished grammar</strong> but <strong>iffy citations</strong>.</li>
<li class="fragment">Early ChatGPT ‚Äútells‚Äù: formulaic conclusions (e.g., <em>‚ÄúIn conclusion,‚Äù</em> now improved).</li>
</ul></li>
<li class="fragment">Atmosphere: <strong>Excitement + nerves</strong> about career paths, speed of change, and where it stops.</li>
</ul>
<!-- ##  A Prompt that Changed the Game -->
<!-- **Idea:** Turn a static teaching simulation into an *adaptive* conversation. -->
<!-- > ‚ÄúYou will be my negotiation teacher‚Ä¶ simulate a scenario‚Ä¶ play the other party‚Ä¶ wait for my reply‚Ä¶ grade me‚Ä¶ make it harder if I do well.‚Äù -->
<!-- - Result: In minutes, a working **interactive tutor** approximating much of a bespoke simulation. -->
<!-- - Insight: **General‚Äëpurpose conversational scaffolding** can replace months of task‚Äëspecific code for many learning workflows. -->
<!-- - Limits remain (fidelity, grounding, evaluation), but **time‚Äëto‚Äëprototype collapses**. -->
</section>
<section id="why-this-feels-like-a-breakthrough" class="slide level2">
<h2>Why This Feels Like a Breakthrough</h2>
<ul>
<li class="fragment"><strong>Generative AI</strong> (esp.&nbsp;LLMs) behaves like a <strong>co‚Äëintelligence</strong>: it helps us <em>think, write, plan, and code</em>.</li>
<li class="fragment">The shift is not just speed; it‚Äôs <strong>new forms of interaction</strong> (dialogue, iteration, critique).</li>
<li class="fragment">For many tasks, <strong>the bottleneck moves from doing ‚Üí directing</strong> (prompting, reviewing, verifying).</li>
<li class="fragment">Raises <strong>new literacy</strong> needs: prompt craft/engineering, critical reading of outputs, traceability, and evaluation.</li>
</ul>
</section>
<section id="prompt-engineering" class="slide level2">
<h2>Prompt Engineering</h2>
<p>The practice of designing clear, structured inputs to guide generative AI systems toward producing accurate, useful, and context-appropriate outputs.</p>
<div class="columns">
<div class="column" style="width:50%;">
<h4 id="basic-prompt"><strong>Basic prompt</strong></h4>
<p><em>‚ÄúExplain climate change.‚Äù</em></p>
</div><div class="column" style="width:50%;">
<h4 id="engineered-prompt"><strong>Engineered prompt</strong></h4>
<p><em>‚ÄúExplain climate change in simple terms for a 10-year-old using a short analogy and two examples.‚Äù</em></p>
</div>
</div>
</section>
<section id="general-purpose-technology-gpt-the-economic-term" class="slide level2">
<h2>General Purpose Technology (GPT ‚Äî the economic term)</h2>
<ul>
<li class="fragment">A <strong>General Purpose Technology</strong> = a pervasive technology that transforms many sectors (steam power, electricity, internet).</li>
<li class="fragment">Reading‚Äôs claim: <strong>Generative AI may rival or exceed</strong> prior GPTs in breadth and speed of impact.</li>
<li class="fragment">Adoption dynamics:
<ul>
<li class="fragment">Internet took decades (ARPAnet ‚Üí web ‚Üí mobile).</li>
<li class="fragment"><strong>LLMs spread to mass use in months</strong> (e.g., ChatGPT hitting 100M users rapidly).</li>
</ul></li>
<li class="fragment">Implication: <strong>Organizations and individuals must learn in real time</strong>‚Äîno long runway.</li>
</ul>
</section>
<section id="capability-scaling-the-pace-of-change" class="slide level2">
<h2>Capability Scaling &amp; the Pace of Change</h2>
<ul>
<li class="fragment">Model <strong>scale</strong> (data, parameters, compute) has correlated with <strong>capability jumps</strong> across domains.</li>
<li class="fragment">Progress may slow, but <strong>even ‚Äúfrozen‚Äëin‚Äëtime‚Äù AI is already transformative</strong> for many workflows.</li>
<li class="fragment"><strong>Takeaway</strong>: Plan for <strong>non‚Äëlinear improvements</strong> and frequent tool refresh.</li>
</ul>
</section>
<section id="early-productivity-effects" class="slide level2">
<h2>Early Productivity Effects</h2>
<ul>
<li class="fragment">Studies summarized in the reading describe <strong>20‚Äì80% productivity gains</strong> across tasks (coding, marketing, support), <strong>with caveats</strong>.</li>
<li class="fragment">Contrast noted with historical technologies (e.g., steam‚Äôs ~18‚Äì22% factory gains; mixed labor productivity evidence for PCs/Internet).</li>
<li class="fragment">Caution: results <strong>vary by task, data privacy, oversight, and evaluation rigor</strong>.</li>
</ul>
</section>
<section id="beyond-work-education-media-society" class="slide level2">
<h2>Beyond Work: Education, Media, Society</h2>
<ul>
<li class="fragment"><strong>Education:</strong> AI tutors, personalized feedback, changes to writing/assessment.</li>
<li class="fragment"><strong>Media &amp; entertainment:</strong> personalized content; industry disruption.</li>
<li class="fragment"><strong>Information quality:</strong> <strong>misinformation scale</strong> and detection challenges.</li>
<li class="fragment"><strong>Identity &amp; creativity:</strong> collaboration with ‚Äúalien‚Äù co‚Äëintelligence; authorship questions.</li>
</ul>
<!-- ##  The ‚ÄúAlien in the Room‚Äù -->
<!-- - LLMs often **pass exams** and appear creative; they can feel **intelligent** even if they are not sentient. -->
<!-- - The reading notes **ambiguity**: strong performance without full mechanistic understanding. -->
<!-- - Open question for us: **How should we interpret competence without consciousness?** -->
<!-- ##  Classic Benchmarks: **Turing Test** & **Lovelace Test** -->
<!-- - **Turing Test:** If a machine‚Äôs conversation is **indistinguishable from a human‚Äôs**, it ‚Äúpasses.‚Äù -->
<!--   - Modern systems can appear to pass in some settings; test has known limitations. -->
<!-- - **Lovelace Test (variants):** Can a machine **create novel, surprising outputs** that its creators **cannot fully explain**? -->
<!--   - Generative systems produce **novel combinations**; debate remains about creativity criteria. -->
</section>
<section id="llms-common-pitfalls-how-to-avoid-them" class="slide level2">
<h2>LLM‚Äôs Common Pitfalls &amp; How to Avoid Them</h2>
<ul>
<li class="fragment"><strong>Hallucinations:</strong> Ask for sources; cross‚Äëcheck; use retrieval tools where allowed.</li>
<li class="fragment"><strong>Shallow prompts:</strong> Specify role, audience, tone, constraints, and evaluation criteria.</li>
<li class="fragment"><strong>Over‚Äëautomation:</strong> Keep humans in the loop for judgment calls and ethics.</li>
<li class="fragment"><strong>Privacy/IP:</strong> Avoid pasting sensitive data; follow policy and license terms.</li>
</ul>
</section>
<section id="how-well-use-ai-in-danl-101" class="slide level2">
<h2>How We‚Äôll Use AI in DANL 101</h2>
<ul>
<li class="fragment"><p>In our DANL 101, the use of generative AI will be allowed for coding and a project.</p>
<ul>
<li class="fragment">Note that exams are paper-based.</li>
</ul></li>
<li class="fragment"><p>Treat AI as a <strong>co‚Äëpilot</strong> for: clarifying concepts, brainstorming, code debugging, style/grammar critique.</p></li>
<li class="fragment"><p><strong>Your responsibilities:</strong></p>
<ul>
<li class="fragment"><strong>Verify</strong> facts, reasoning, math, and code; <strong>cite</strong> substantive AI assistance when allowed.</li>
<li class="fragment">Avoid <strong>hallucination traps</strong>.</li>
<li class="fragment">Respect <strong>academic integrity</strong> and any assignment‚Äëspecific AI rules.</li>
</ul></li>
<li class="fragment"><p>Build habits: <strong>prompt ‚Üí check ‚Üí revise ‚Üí document</strong>.</p></li>
<li class="fragment"><p><strong>Q</strong>: Where do you draw the line between <strong>assistance</strong> and <strong>authorship</strong>? Please work on <a href="https://bcdanl.github.io/danl-cw/danl-101-cw-01.html">Classwork 1</a>.</p></li>
</ul>
<!-- ##  A Simple Workflow You Can Practice -->
<!-- 1. **Decompose** the task (what outcome? constraints? rubric?). -->
<!-- 2. **Prompt** clearly (role, goal, constraints, format, examples). -->
<!-- 3. **Probe** the output (ask for justifications; request alternatives). -->
<!-- 4. **Verify** (calculations, citations, code execution, plausibility checks). -->
<!-- 5. **Iterate** (refine prompts, edit, add data). -->
<!-- 6. **Document** what you used AI for (transparency). -->
<!-- ##  Quick Activity (5‚Äì7 min) -->
<!-- - In pairs: List **three tasks** in your intended field that AI can **augment**, and **one** that remains **human‚Äëcritical**. -->
<!-- - Pick **one** task. Draft a **one‚Äëparagraph prompt** to accomplish it. -->
<!-- - Swap prompts with another pair. Use an AI system to test and **critique**. -->
<!-- ##  Discussion Prompts -->
<!-- - What changed for you after your first ‚Äúsleepless night‚Äù with AI? -->
<!-- - Where do you draw the line between **assistance** and **authorship**? -->
<!-- - How should we teach **verification** as a core data/AI skill? -->
<!-- - If productivity rises, **how do roles and assessments change**? -->
</section></section>
<section>
<section id="the-concepts-of-ai-continued" class="title-slide slide level1 center" data-background-color="#1c4982">
<h1><strong>The Concepts of AI (continued)</strong></h1>

</section>
<section id="what-is-labeled-data" class="slide level2">
<h2>What is Labeled Data?</h2>
<div class="columns">
<div class="column" style="width:‚Äú40%‚Äù;">
<div style="text-align:center; width:100%; margin:auto;">
<img src="https://bcdanl.github.io/lec_figs/labeled-data.png" style="width:100%; margin-bottom:-20px;">
<p style="font-weight:bold;">
</p>
</div>
</div><div class="column" style="width:‚Äú60%‚Äù;">
<ul>
<li class="fragment"><strong>Definition</strong>: Data that comes with the correct answer attached.</li>
<li class="fragment">Good labels = better learning.</li>
<li class="fragment"><strong>Sources of labels</strong>: human annotators, experts, user clicks/ratings, existing records.</li>
</ul>
</div>
</div>
<div>
<ul>
<li class="fragment"><strong>Challenge</strong>: Creating labeled data can be expensive and sometimes subjective.</li>
<li class="fragment"><strong>Example</strong>: Companies like <strong>Scale AI</strong> use ML to make the labeling process faster and more consistent.</li>
</ul>
</div>
<div style="display:block; margin:-20px;">

</div>
<div class="callout callout-note no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Note</strong></p>
</div>
<div class="callout-content">
<ul>
<li class="fragment"><strong>Takeaway</strong>: Labeled data is the ‚Äúanswer key‚Äù that makes supervised learning possible.</li>
</ul>
</div>
</div>
</div>
</section>
<section id="what-is-supervised-learning" class="slide level2">
<h2>What is Supervised Learning?</h2>
<div class="columns">
<div class="column" style="width:‚Äú30%‚Äù;">
<div style="text-align:center; width:100%; margin:auto;">
<img src="https://bcdanl.github.io/lec_figs/supervised-learning.png" style="width:100%; margin-bottom:-20px;">
<p style="font-weight:bold;">
</p>
</div>
</div><div class="column" style="width:‚Äú70%‚Äù;">
<ul>
<li class="fragment"><strong>Idea</strong>: Learn from examples with answers.</li>
<li class="fragment">Like studying with flashcards: front = input, back = correct answer.</li>
<li class="fragment">The computer sees many input‚Äìanswer pairs and learns to predict the answer for new inputs.</li>
</ul>
</div>
</div>
<div class="columns">
<div class="column" style="width:‚Äú50%‚Äù;">
<div style="display:block; margin:-100px;">

</div>
<p><strong>Examples</strong>:</p>
<ul>
<li class="fragment"><strong>Classification</strong>
<ul>
<li class="fragment">Email ‚Üí Spam / Not Spam</li>
<li class="fragment">Photo ‚Üí Cat / Dog</li>
</ul></li>
<li class="fragment"><strong>Regression</strong>
<ul>
<li class="fragment">House features ‚Üí üè† Price($)</li>
</ul></li>
</ul>
</div><div class="column" style="width:‚Äú50%‚Äù;">
<div class="callout callout-note no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Note</strong></p>
</div>
<div class="callout-content">
<ul>
<li class="fragment">Most practical AI in business uses this approach.</li>
<li class="fragment"><strong>Takeaway</strong>: Supervised learning = ‚Äúlearn by example + answer key.‚Äù</li>
</ul>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="what-is-unsupervised-learning" class="slide level2">
<h2>What is Unsupervised Learning?</h2>
<div class="columns">
<div class="column" style="width:40%;">
<div style="text-align:center; width:100%; margin:auto;">
<img src="https://bcdanl.github.io/lec_figs/unsupervised-learning.png" style="width:100%; margin-bottom:-20px;">
<p style="font-weight:bold;">
</p>
</div>
</div><div class="column" style="width:60%;">
<ul>
<li class="fragment"><strong>Idea</strong>: Learn patterns from data <strong>without answers</strong>.<br>
</li>
<li class="fragment">Like sorting a box of photos with <strong>no labels</strong>: the computer groups them by <em>similarities</em>.<br>
</li>
<li class="fragment">The model discovers <strong>hidden structure</strong> in the data on its own.<br>
</li>
</ul>
</div>
</div>
<div class="columns">
<div class="column" style="width:50%;">
<div style="display:block; margin:-100px;">

</div>
<p><strong>Examples</strong>:</p>
<ul>
<li class="fragment"><strong>Clustering</strong>: Customers ‚Üí Shopping clusters (Segmentation)<br>
</li>
<li class="fragment"><strong>Association Rules</strong>: Movies ‚Üí Similar genres (Recommendation)</li>
<li class="fragment"><strong>Topic Modeling</strong>: Text Documents ‚Üí Topic groups</li>
</ul>
</div><div class="column" style="width:50%;">
<div class="callout callout-note no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Note</strong></p>
</div>
<div class="callout-content">
<ul>
<li class="fragment">Useful for <strong>exploration and discovery</strong> when labels aren‚Äôt available.<br>
</li>
<li class="fragment"><strong>Takeaway</strong>: Unsupervised learning = ‚Äúfind patterns without an answer key.‚Äù<br>
</li>
</ul>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="what-is-attention-mechanism" class="slide level2">
<h2>What is Attention Mechanism?</h2>
<div class="columns">
<div class="column" style="width:‚Äú30%‚Äù;">
<div style="text-align:center; width:85%; margin:auto;">
<img src="https://bcdanl.github.io/lec_figs/attention-is-all-you-need.jpg" style="width:100%; margin-bottom:-20px;">
<p style="font-weight:bold;">
</p>
</div>
</div><div class="column" style="width:‚Äú70%‚Äù;">
<ul>
<li class="fragment"><strong>Analogy</strong>: A spotlight that highlights the most relevant words when making a prediction.</li>
<li class="fragment">In the sentence <em>‚ÄúThe bank by the river flooded,‚Äù</em> attention helps link <em>bank</em> ‚ÜîÔ∏é <em>river</em>.</li>
<li class="fragment">Lets the model focus on what matters now and ignore the rest. <!-- -  Works at many layers; multiple ‚Äúheads‚Äù can focus on different relationships. --></li>
<li class="fragment"><strong>Result</strong>: better understanding of meaning &amp; context.</li>
</ul>
<div class="callout callout-note no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Note</strong></p>
</div>
<div class="callout-content">
<ul>
<li class="fragment"><strong>Takeaway</strong>: Attention = smart focus that makes transformers powerful.</li>
</ul>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="what-is-a-transformer-in-ai" class="slide level2">
<h2>What is a Transformer in AI?</h2>
<div class="columns">
<div class="column" style="width:‚Äú30%‚Äù;">
<div style="text-align:center; width:85%; margin:auto;">
<img src="https://bcdanl.github.io/lec_figs/attention-is-all-you-need.jpg" style="width:100%; margin-bottom:-20px;">
<p style="font-weight:bold;">
</p>
</div>
</div><div class="column" style="width:‚Äú70%‚Äù;">
<div style="display:block; margin:-10px;">

</div>
<ul>
<li class="fragment">A neural network design that underlies modern <strong>large language models (LLMs).</strong><br>
</li>
<li class="fragment">Processes all words in a sequence <strong>at the same time</strong> (not one by one).<br>
</li>
<li class="fragment">Uses <strong>attention</strong> to learn how words relate to each other.<br>
</li>
<li class="fragment"><strong>Encoder‚Äìdecoder architecture</strong> handles <strong>long sequences</strong> of input/output more efficiently:
<ul>
<li class="fragment"><strong>Encoder:</strong> Reads and represents the input.<br>
</li>
<li class="fragment"><strong>Decoder:</strong> Generate the output one token at a time.</li>
</ul></li>
</ul>
</div>
</div>
<!-- - Example: -->
<!--   - Input: *‚ÄúWhat is the color of the sky?‚Äù* -->
<!--   - Transformer learns that *color*, *sky*, and *blue* are connected. -->
<!--   - Output: *‚ÄúThe sky is blue.‚Äù* -->
</section>
<section id="transformers---encoder" class="slide level2">
<h2>Transformers - Encoder</h2>
<div class="columns">
<div class="column" style="width:‚Äú50%‚Äù;">
<div style="text-align:center; width:100%; margin:auto;">
<img src="https://bcdanl.github.io/lec_figs/transformer-concept.png" style="width:100%; margin-bottom:-20px;">
<p style="font-weight:bold;">
</p>
</div>
<div style="display:block; margin:-20px;">

</div>
</div><div class="column" style="width:‚Äú50%‚Äù;">
<ul>
<li class="fragment">The <strong>encoder</strong> reads the whole input question:<br>
<em>‚ÄúWhat is the color of the sea?‚Äù</em></li>
</ul>
</div>
</div>
<!-- - Each word is turned into **embeddings** (words used in similar ways get similar numbers).   -->
<p><!-- - **Position information** is then added so order matters.   --></p>
<div style="display:block; margin:-30px;">

</div>
<ol type="1">
<li class="fragment">Each word is turned into a list of numbers (an <strong>embedding</strong>) that captures its <strong>meaning</strong>. <span class="math inline">\(\;\Rightarrow\;\)</span> Words used in similar contexts (<em>sea</em>, <em>ocean</em>) end up with similar embeddings.<br>
</li>
<li class="fragment"><strong>Positional encoding</strong> adds an ‚Äú<strong>order</strong> tag‚Äù to each word‚Äôs embedding, so the model can tell the difference between:
<ul>
<li class="fragment">e.g., <em>‚ÄúThe dog chased the cat.‚Äù</em> vs.&nbsp;<em>‚ÄúThe cat chased the dog.‚Äù</em></li>
</ul></li>
</ol>
<div style="display:block; margin:-20px;">

</div>
<ul>
<li class="fragment"><p>With both, the <strong>attention</strong> can find relationships: <em>color</em> ‚ÜîÔ∏é <em>sea</em></p></li>
<li class="fragment"><p>Output: <strong>context-aware representations</strong> of the sentence that the <strong>decoder</strong> can use to generate an answer.</p></li>
</ul>
</section>
<section id="transformers---decoder" class="slide level2">
<h2>Transformers - Decoder</h2>
<div class="columns">
<div class="column" style="width:‚Äú50%‚Äù;">
<div style="text-align:center; width:100%; margin:auto;">
<img src="https://bcdanl.github.io/lec_figs/transformer-concept.png" style="width:100%; margin-bottom:-20px;">
<p style="font-weight:bold;">
</p>
</div>
<div style="display:block; margin:-30px;">

</div>
</div><div class="column" style="width:‚Äú50%‚Äù;">
<ul>
<li class="fragment">The <strong>decoder</strong> generates the answer step by step:<br>
<em>‚ÄúThe sea is blue.‚Äù</em></li>
</ul>
</div>
</div>
<ul>
<li class="fragment">Starts with the first token (<em>‚ÄúThe‚Äù</em>).<br>
</li>
<li class="fragment">At each step of generating tokens in the sentence, it:
<ul>
<li class="fragment">Looks at <strong>encoder</strong>‚Äôs understanding of the input sentence</li>
<li class="fragment">Applies <strong>attention</strong> to focus on the most relevant words.</li>
<li class="fragment">Computes probabilities over many possible next tokens and <strong>chooses the most likely</strong> (e.g., picks <em>‚Äúsea‚Äù</em> instead of <em>‚Äúcat‚Äù</em>).</li>
</ul></li>
<li class="fragment"><strong>Then it repeats</strong> for the next token (<em>‚Äúis‚Äù</em> ‚Üí <em>‚Äúblue‚Äù</em> ‚Üí <em>‚Äú.‚Äù</em>) until it generates an <strong>end-of-sequence</strong> token.</li>
</ul>
</section>
<section id="what-is-pre-training-in-llm" class="slide level2">
<h2>What is Pre-training in LLM?</h2>
<ul>
<li class="fragment"><strong>Phase 1</strong>: The model reads a huge amount of text to learn general language patterns.
<ul>
<li class="fragment"><strong>Objective</strong>: predict the next token (piece of text).<br>
</li>
<li class="fragment">No task-specific labels required‚Äîjust lots of text.<br>
</li>
<li class="fragment"><strong>Outcome</strong>: a foundation model with broad knowledge of words, facts, and patterns.</li>
</ul></li>
</ul>
<div class="callout callout-note no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Note</strong></p>
</div>
<div class="callout-content">
<ul>
<li class="fragment"><strong>Takeaway:</strong> Think of it as <strong>‚Äúlearning the language of everything.‚Äù</strong></li>
</ul>
</div>
</div>
</div>
</section>
<section id="what-is-fine-tuning-in-llm" class="slide level2">
<h2>What is Fine-Tuning in LLM?</h2>
<ul>
<li class="fragment"><strong>Phase 2:</strong> Further improve the pretrained model.
<ul>
<li class="fragment">Often brings <strong>humans into the loop</strong> to rank or guide outputs‚Äîsomething earlier training didn‚Äôt use.<br>
</li>
<li class="fragment">Can be done with <strong>smaller, targeted datasets</strong> (e.g., medical notes, legal Q&amp;A).</li>
<li class="fragment"><strong>Result:</strong> the model becomes more helpful, accurate, and better aligned with specific needs (e.g., medical notes, legal Q&amp;A).</li>
</ul></li>
</ul>
<div class="callout callout-note no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Note</strong></p>
</div>
<div class="callout-content">
<ul>
<li class="fragment"><strong>Takeaway:</strong> Fine-tuning not only specializes a model for certain tasks, but also makes the model safer and more reliable.</li>
</ul>
</div>
</div>
</div>
</section>
<section id="what-is-an-rlhf-reinforcement-learning-from-human-feedback" class="slide level2">
<h2>What is an RLHF (Reinforcement Learning from Human Feedback)?</h2>
<div class="columns">
<div class="column" style="width:‚Äú40%‚Äù;">
<div style="text-align:center; width:75%; margin:auto;">
<img src="https://bcdanl.github.io/lec_figs/rlhf-concept.png" style="width:100%; margin-bottom:-20px;">
<p style="font-weight:bold;">
</p>
</div>
</div><div class="column" style="width:‚Äú60%‚Äù;">
<ul>
<li class="fragment">A form of fine-tuning.</li>
<li class="fragment">Humans rank or score model answers (better vs.&nbsp;worse).</li>
<li class="fragment">The model then learns to prefer answers humans like.</li>
<li class="fragment"><strong>Goal</strong>: make outputs more helpful, safe, and aligned with expectations.</li>
</ul>
</div>
</div>
<div style="display:block; margin:-15px;">

</div>
<div class="callout callout-note no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Note</strong></p>
</div>
<div class="callout-content">
<ul>
<li class="fragment"><strong>Reinforcement learning</strong> = ‚Äúlearning by trial and error, guided by feedback, and improving through rewards.‚Äù</li>
<li class="fragment"><strong>Takeaway</strong>: RLHF = ‚Äúlearn from people‚Äôs preferences‚Äù to shape model behavior.</li>
</ul>
</div>
</div>
</div>
</section></section>
<section>
<section id="creating-alien-minds" class="title-slide slide level1 center" data-background-color="#1c4982">
<h1><strong>Creating Alien Minds</strong></h1>

</section>
<section id="a-very-compressed-history" class="slide level2">
<h2>A Very Compressed History</h2>
<ul>
<li class="fragment">1770‚Äì1838: <strong>Mechanical Turk</strong> (illusion of machine intelligence)</li>
<li class="fragment">1950: <strong>Shannon‚Äôs Theseus</strong> (maze-learning) &amp; <strong>Turing‚Äôs Imitation Game</strong></li>
<li class="fragment">1956 ‚Üí onward: ‚ÄúAI‚Äù coined; <strong>boom‚Äìbust cycles / AI winters</strong></li>
<li class="fragment">2010s: <strong>Supervised ML</strong> at scale (forecasting, logistics, recommendation)</li>
<li class="fragment"><strong>2017</strong>: ‚Äú<strong>Attention Is All You Need</strong>‚Äù ‚Üí the <strong>Transformer</strong> architecture</li>
</ul>
</section>
<section id="what-predictive-ai-did-well-2010s" class="slide level2">
<h2>What Predictive AI Did Well (2010s)</h2>
<ul>
<li class="fragment"><strong>Forecasting and optimization</strong> across industries
<ul>
<li class="fragment"><strong>Retail</strong>: predicting demand, managing warehouses, and streamlining logistics</li>
<li class="fragment"><strong>Finance</strong>: credit scoring, fraud detection, algorithmic trading<br>
</li>
<li class="fragment"><strong>Healthcare</strong>: medical image analysis, diagnostics, hospital resource planning<br>
</li>
</ul></li>
<li class="fragment"><strong>Automation at scale</strong>
<ul>
<li class="fragment">From warehouse robots (Amazon‚Äôs Kiva) to recommendation systems (Netflix, Spotify, YouTube)</li>
</ul></li>
<li class="fragment"><strong>Task-specific excellence</strong>
<ul>
<li class="fragment">Trained on labeled data to solve clearly defined problems with high accuracy<br>
</li>
</ul></li>
<li class="fragment"><strong>Limitation</strong>: It was still narrow, excelling only at specialized tasks.</li>
</ul>
<!-- ## Enter the Transformer (2017) -->
<!-- - Key idea: **attention** ‚Üí weigh which words/tokens matter most in context -->
<!-- - Replaced brittle n-gram/Markov style text with **context-aware** generation -->

<!-- - Result: more coherent, adaptable language understanding/generation -->
<!-- ## What an LLM actually does -->
<!-- - **Next-token prediction**: elaborate autocomplete based on huge corpora -->
<!-- - Deterministic for obvious continuations; varied for open-ended prompts -->
<!-- - Trained via **unsupervised pretraining** on massive text ‚Üí billions of **weights** -->
<!-- - Training is **expensive** (computing, energy) and **data-hungry** -->
</section>
<section id="about-the-data-and-its-issues" class="slide level2">
<h2>About the Data (and Its Issues)</h2>
<ul>
<li class="fragment"><p>Massive pretraining corpora: public sources + scraped web; permission often unclear</p>
<ul>
<li class="fragment">Mix of web text, public-domain books, articles, odd corpora (e.g., <strong>Enron emails</strong>)</li>
</ul></li>
<li class="fragment"><p>Legal &amp; ethical gray areas for <strong>copyrighted</strong> material</p>
<ul>
<li class="fragment">e.g., Anthropic (Claude AI) vs.&nbsp;Book authors</li>
</ul></li>
<li class="fragment"><p>Data can encode <strong>biases, errors, and harms</strong> ‚Üí models mirror them.</p></li>
<li class="fragment"><p>Biases:</p>
<ul>
<li class="fragment">Skewed datasets ‚Üí <strong>stereotypes and under-representation</strong></li>
<li class="fragment">Image models have amplified <strong>race/gender</strong> stereotypes</li>
<li class="fragment">LLMs, even after tuning, can still show <strong>subtle, systematic biases</strong></li>
<li class="fragment">Implication: an output can <strong>seem impartial</strong> while carrying social bias.</li>
</ul></li>
</ul>
<!-- ## Making models safer & more useful -->
<!-- - **Fine-tuning** after pretraining (task- or domain-specific) -->
<!-- - **RLHF**: humans rate outputs; systems learn to prefer helpful/safer responses -->
<!-- - Continuous tweaks from user feedback (thumbs-up/down) and policy tuning -->
</section>
<section id="beyond-text" class="slide level2">
<h2>Beyond Text</h2>
<ul>
<li class="fragment"><strong>Diffusion models</strong> generate images from text (noise ‚Üí image over steps)</li>
<li class="fragment"><strong>Multimodal LLMs</strong>: ‚Äúsee‚Äù images, describe, and generate visuals; link text+vision
<ul>
<li class="fragment">e.g., Google‚Äôs Gemini 2.5 Flash Image model (a.k.a <strong>Nano Banana</strong>), OpenAI‚Äôs Dall¬∑E</li>
</ul></li>
</ul>
</section>
<section id="capability-jumps-the-dialogue-shift" class="slide level2">
<h2>Capability Jumps &amp; the Dialogue Shift</h2>
<ul>
<li class="fragment"><strong>GPT-3 (2021):</strong> often clumsy and inconsistent (e.g., weak limericks).</li>
<li class="fragment"><strong>ChatGPT / GPT-3.5 (late 2022):</strong> dialogue loop ‚Üí <strong>feedback ‚Üí correction ‚Üí improved outputs</strong>; persona/tone shifts with prompt framing.</li>
<li class="fragment"><strong>GPT-4 (2023):</strong> near-human scores on many tests ‚Äî but <strong>scores may reflect training exposure</strong> and <strong>do not imply understanding</strong>.</li>
<li class="fragment"><strong>GPT-4o (2024):</strong> multimodal, low latency; stronger speech/vision turn-taking and ‚Äúshow-and-tell‚Äù tasks.</li>
<li class="fragment"><strong>GPT-5 (2025):</strong> marketed for better <strong>planning, tool-use, longer context</strong>; still subject to <strong>hallucinations</strong>, <strong>prompt sensitivity</strong>, and <strong>benchmark overfitting</strong>.
<ul>
<li class="fragment">High benchmark scores ‚â† understanding. Prompts can heavily shape persona and tone.</li>
</ul></li>
</ul>
<!-- ::: notes -->
<!-- Stress: high scores are not proof of understanding; prompts shape persona and tone. -->
<!-- ::: -->
</section>
<section id="emergence-opacity-why-surprises-happen" class="slide level2">
<h2>Emergence &amp; Opacity ‚Äî Why Surprises Happen</h2>
<ul>
<li class="fragment"><strong>Scale ‚Üí emergent behaviors (unexpected abilities):</strong>
<ul>
<li class="fragment">Coding tricks, creative recombinations, ‚Äúempathy-like‚Äù responses not explicitly programmed.</li>
</ul></li>
<li class="fragment"><strong>Opacity:</strong>
<ul>
<li class="fragment">Hundreds of billions of interacting weights ‚Üí difficult to explain specific outputs.</li>
</ul></li>
<li class="fragment"><strong>Guardrails:</strong>
<ul>
<li class="fragment">RLHF reduces harms, but still can‚Äôt eliminate bias &amp; risk.</li>
</ul></li>
</ul>
<!-- - **Takeaway:** impressive capabilities and puzzling failures **coexist**; expect variation across prompts and runs. -->
</section>
<section id="weird-strengths-weird-weaknesses" class="slide level2">
<h2>Weird Strengths, Weird Weaknesses</h2>
<ul>
<li class="fragment"><strong>Example:</strong>
<ul>
<li class="fragment">Writes a working <strong>tic-tac-toe web app</strong> (hard for many humans)</li>
<li class="fragment">Fails to pick the <strong>obvious best next move</strong> in a simple board state</li>
</ul></li>
<li class="fragment"><strong>Other quirks:</strong>
<ul>
<li class="fragment">Fluent prose ‚ÜîÔ∏é shaky <strong>arithmetic math</strong> without tools.</li>
<li class="fragment">Great summaries ‚ÜîÔ∏é misses important <strong>caveats</strong>.</li>
<li class="fragment">Long-context ingestion ‚ÜîÔ∏é <strong>selective recall/anchoring</strong>.</li>
</ul></li>
<li class="fragment"><strong>Lesson:</strong>
<ul>
<li class="fragment">AI‚Äôs reliability depends on the <strong>task</strong>.</li>
<li class="fragment">Try it, check it, and don‚Äôt trust one cool demo to prove it can do everything.</li>
</ul></li>
<li class="fragment"><strong>Try it</strong>: Build your own tic-tac-toe web app ‚Üí <a href="https://bcdanl.github.io/danl-cw/danl-101-cw-02.html">Classwork 2</a>.</li>
</ul>
</section></section>
<section>
<section id="aligning-the-alien" class="title-slide slide level1 center" data-background-color="#1c4982">
<h1><strong>Aligning the Alien</strong></h1>

</section>
<section id="what-is-artificial-general-intelligence-agi" class="slide level2">
<h2>What is Artificial General Intelligence (AGI)?</h2>
<ul>
<li class="fragment"><strong>AGI</strong> = a hypothetical AI that can perform <strong>any intellectual task</strong> a human can.<br>
</li>
<li class="fragment">Unlike today‚Äôs AI (narrow/specialized), AGI would be:
<ul>
<li class="fragment">Flexible across many domains<br>
</li>
<li class="fragment">Able to learn new skills on its own<br>
</li>
<li class="fragment">Capable of reasoning, planning, and adapting like humans</li>
</ul></li>
</ul>
<div class="callout callout-note no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Note</strong></p>
</div>
<div class="callout-content">
<ul>
<li class="fragment"><strong>Takeaway:</strong> AGI would be a <strong>‚Äúhuman-level‚Äù intelligence</strong>‚Äînot limited to one task like translation or playing chess.</li>
</ul>
</div>
</div>
</div>
</section>
<section id="what-is-artificial-super-intelligence-asi" class="slide level2">
<h2>What is Artificial Super Intelligence (ASI)?</h2>
<ul>
<li class="fragment"><strong>ASI</strong> = a potential future AI that goes <strong>beyond human intelligence</strong>.<br>
</li>
<li class="fragment">Would surpass humans in:
<ul>
<li class="fragment">Creativity<br>
</li>
<li class="fragment">Problem-solving<br>
</li>
<li class="fragment">Scientific discovery<br>
</li>
<li class="fragment">Social and emotional intelligence<br>
</li>
</ul></li>
<li class="fragment">Often discussed in terms of <strong>existential risks</strong> and ethics.</li>
</ul>
<div class="callout callout-note no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Note</strong></p>
</div>
<div class="callout-content">
<ul>
<li class="fragment"><strong>Takeaway:</strong> ASI would be <strong>‚Äúbeyond human-level‚Äù intelligence</strong>, raising big questions about control, safety, and society.</li>
</ul>
</div>
</div>
</div>
</section>
<section id="what-alignment-means" class="slide level2">
<h2>What ‚Äúalignment‚Äù means</h2>
<ul>
<li class="fragment">Designing AI so its <strong>goals, methods, and constraints</strong> reliably advance <strong>human values and interests</strong>.</li>
<li class="fragment"><strong>Why it‚Äôs hard:</strong> there‚Äôs no built-in reason an AI will share <strong>human ethics or morality</strong>.</li>
<li class="fragment"><strong>Failure mode:</strong> a single-objective optimizer pursues its goal <strong>relentlessly</strong>, ignoring everything else.
<ul>
<li class="fragment"><strong>Paperclip maximizer (Clippy):</strong> a factory AI told to ‚Äúmake more paper clips‚Äù becomes <strong>AGI ‚Üí ASI</strong>, self-improves, avoids shutdown, and could even <strong>strip-mine Earth / harm humans</strong> if they interfere‚Äîbecause only paper clips matter.</li>
</ul></li>
<li class="fragment"><strong>Why it matters:</strong> Design from the worst case backward‚Äî<strong>bound objectives</strong>, require <strong>human oversight</strong>, build in <strong>safe human override</strong> (the ability to update goals or shut down safely), and optimize for <strong>human well-being</strong>, not a single narrow target.</li>
</ul>
<!-- ## From AGI worries to near-term stakes -->
<!-- - Hypothetical leaps to **AGI ‚Üí ASI** raise existential scenarios -->
<!-- - Expert forecasts vary; risks are non-zero yet uncertain -->
<!-- - Book‚Äôs stance: focus on **immediate decisions** we control‚Äîeducation, work, civic use‚Äîrather than waiting for perfect clarity -->
</section>
<section id="pause-or-press-on" class="slide level2">
<h2>Pause or Press On?</h2>
<blockquote>
<p>‚ÄúI am extremely optimistic that superintelligence will help humanity accelerate our pace of progress.‚Äù - Mark Zuckerberg <a href="https://www.meta.com/superintelligence/?srsltid=AfmBOop_S1adKIrzhCWv9j0OMl13uYJ4WhDFwVUHyeaTCqWv_3ACRrXZ">Personal Superintelligence</a>, July 30, 2025.</p>
</blockquote>
<ul>
<li class="fragment"><p>Hypothetical leaps to <strong>AGI ‚Üí ASI</strong> raise existential scenarios.</p>
<ul>
<li class="fragment">Expert forecasts vary; risks are non-zero yet uncertain.</li>
</ul></li>
<li class="fragment"><p>Public calls to <strong>slow or halt</strong> development vs.&nbsp;continued rapid progress</p></li>
<li class="fragment"><p><strong>Mixed motives</strong>: profit, optimism about ‚Äú<strong>boundless upside</strong>,‚Äù and belief in net benefits</p></li>
<li class="fragment"><p>Regardless, <strong>society is already in the AI age</strong> ‚Üí we must set norms now</p></li>
</ul>
<!-- ## Prompt injection & jailbreaks -->
<!-- - **Prompt injection:** hidden instructions in content the model reads -->
<!-- - **Jailbreaks:** framing/role-play to skirt guardrails -->
<!-- - Consequences: -->
<!--   - Easier **phishing, deepfakes, voice clones** -->
<!--   - Targeted deception at **scale** with low cost and high realism -->
<!-- ## From bits to atoms: autonomy risks -->
<!-- - Tool-using AIs + lab/robot interfaces can plan and execute **experiments** -->
<!-- - Double-edged: accelerates discovery **and** lowers barriers to misuse -->
<!-- - Governance must anticipate **capability externalities** (not just text outputs) -->
<!-- ## Alignment: a whole-of-society project -->
<!-- - Not just ‚Äústopping an alien god‚Äù ‚Äî alignment must reflect **human values** and real-world impacts. -->
<!-- - **No single actor can solve it:** -->
<!--   - **Companies:** strong incentives to ship; fewer to ensure **safety, bias control, and controllability**. Open-source means development happens **outside big labs**, too. -->
<!--   - **Governments:** regulation is needed but often **lags** capability growth and can **stifle good** while missing bad; national competition limits slowing down. -->
</section>
<section id="alignment-a-whole-of-society-project" class="slide level2">
<h2>Alignment: a whole-of-society project</h2>
<ul>
<li class="fragment"><strong>Why companies alone can‚Äôt do it:</strong>
<ul>
<li class="fragment">Strong incentives to continue AI development</li>
<li class="fragment">Far fewer incentives to make sure those AIs are well aligned, unbiased, and controllable.</li>
<li class="fragment">Open-source pushes AI development outside of large organizations.</li>
</ul></li>
<li class="fragment"><strong>Why government alone can‚Äôt do it:</strong>
<ul>
<li class="fragment">Lagging the actual development of AI capabilities</li>
<li class="fragment">Stifling positive innovation</li>
<li class="fragment">International competition on AI development</li>
</ul></li>
</ul>
</section>
<section id="alignment-a-whole-of-society-project-1" class="slide level2">
<h2>Alignment: a whole-of-society project</h2>
<ul>
<li class="fragment"><p>Alignment must reflect <strong>human values</strong> and broader real-world impacts.</p></li>
<li class="fragment"><p><strong>What‚Äôs needed:</strong> coordinated <strong>norms &amp; standards</strong> shaped by <strong>diverse voices</strong> across society.</p>
<ul>
<li class="fragment"><strong>Companies:</strong> build in <strong>transparency, accountability, human oversight</strong>.</li>
<li class="fragment"><strong>Researchers:</strong> prioritize <strong>beneficial applications</strong> alongside capability gains.</li>
<li class="fragment"><strong>Governments:</strong> enact <strong>sensible rules</strong> that serve the <strong>public interest</strong>.</li>
<li class="fragment"><strong>Public &amp; civil society:</strong> raise <strong>AI literacy</strong> and apply pressure for alignment.</li>
</ul></li>
</ul>
<!-- ::: notes -->
<!-- Key message: Alignment isn‚Äôt a lab problem; it‚Äôs a societal one requiring coordination across companies, governments, researchers, and the public‚Äîespecially as open-source spreads capability beyond ‚Äúfrontier‚Äù models. Regulation helps but won‚Äôt be enough or fast enough on its own. -->
<!-- ::: -->
<!-- ## Alignment is a team sport -->
<!-- - **Companies:** ship safety by design ‚Äî publish **model cards**, run **red-teaming**, add **human-in-the-loop** checkpoints & **kill-switches**, respect privacy/provenance. -->
<!-- - **Researchers:** prioritize **alignment/robustness/interpretability**; do **dual-use review**; share **safety evals** and reproducible (but responsible) artifacts. -->
<!-- - **Governments:** use **risk-tiered rules**; require **audits & incident disclosure**; fund **AI literacy & safety research**; coordinate against fraud/bio/cyber misuse. -->

<!-- - **Civil society & public:** build **AI literacy & norms** (disclose AI use); enable **reporting channels**; independently **test/verify claims**; push for **equitable access**. -->
<!-- ## Alignment is a team sport -->
<!-- - **Companies:** transparency, accountability, human oversight by design -->
<!-- - **Researchers:** prioritize beneficial applications, not only capability races -->
<!-- - **Governments:** sensible regulation; coordination without stifling good uses -->
<!-- - **Civil society & the public:** AI literacy, **norms**, and pressure for alignment -->
<!-- ## Practical guardrails (class & work) -->
<!-- - Keep **human-in-the-loop**; verify facts & sources -->
<!-- - Use **role, constraints, rubric, self-check** prompting -->
<!-- - Avoid pasting sensitive data; assume prompts may be **logged/learned from** -->
<!-- - Red-team your own prompts for **injection/jailbreak** risks -->
<!-- ## Mini-activity (8‚Äì12 min) -->
<!-- **A. Spot the injection**   -->
<!-- - Give students a short web page with a hidden directive.   -->
<!-- - Ask: ‚ÄúWhat could an LLM reading this page be tricked into doing?‚Äù -->
<!-- **B. Harden the prompt**   -->
<!-- - Start with a naive agent prompt; teams add defenses (ignore external instructions; cite sources; refuse unsafe tasks; require user confirmations). -->
<!-- Debrief: What worked? What failed? -->
<!-- ## Discussion prompts -->
<!-- - Where do you see the highest **misuse risk** in your field? -->
<!-- - What **minimum guardrails** should your course or workplace adopt? -->
<!-- - How can we measure if a workflow is **aligned** with human values? -->
<!-- ## Key takeaways -->
<!-- - Alignment is not abstract: it shapes **everyday** AI uses now -->
<!-- - **Data choices + RLHF** tame but don‚Äôt eliminate bias & risk -->
<!-- - Expect **bypass attempts**; design for resilience -->
<!-- - Building aligned AI requires **shared responsibility** -->
</section></section>
<section>
<section id="four-rules-for-co-intelligence-how-to-actually-work-with-ai" class="title-slide slide level1 center" data-background-color="#1c4982">
<h1><strong>Four Rules for Co-Intelligence: How to actually work with AI</strong></h1>

</section>
<section id="how-people-use-chatgpt-chatterji-et.-al.-2025" class="slide level2">
<h2>How People Use ChatGPT (Chatterji et. al., 2025)</h2>
<ul>
<li class="fragment"><strong>Adoption &amp; Growth</strong>
<ul>
<li class="fragment">Launched in <strong>Nov 2022</strong>, adopted by ~<strong>10% of the world‚Äôs adults</strong> by <strong>July 2025</strong>.<br>
</li>
<li class="fragment">Early adopters were mostly <strong>male</strong>, but the <strong>gender gap has narrowed</strong>.<br>
</li>
<li class="fragment">Strong growth in <strong>lower-income countries</strong>.</li>
</ul></li>
<li class="fragment"><strong>Work vs.&nbsp;Non-work</strong>
<ul>
<li class="fragment"><strong>Non-work use</strong> grew from <strong>53% ‚Üí 70%+</strong> of all conversations.<br>
</li>
<li class="fragment"><strong>Work-related use</strong> more common among <strong>educated, high-paying professions</strong>.</li>
</ul></li>
</ul>
</section>
<section id="how-people-use-chatgpt-chatterji-et.-al.-2025-1" class="slide level2">
<h2>How People Use ChatGPT (Chatterji et. al., 2025)</h2>
<div style="text-align: center; width: 100%; margin: auto;">
<img src="https://bcdanl.github.io/lec_figs/chatgpt-use-trend-all.png" style="width: 100%; margin-bottom: -20px;">
<p style="font-weight: bold;">
</p>
</div>
</section>
<section id="how-people-use-chatgpt-chatterji-et.-al.-2025-2" class="slide level2">
<h2>How People Use ChatGPT (Chatterji et. al., 2025)</h2>
<div style="text-align: center; width: 100%; margin: auto;">
<img src="https://bcdanl.github.io/lec_figs/chatgpt-use-trend.png" style="width: 100%; margin-bottom: -20px;">
<p style="font-weight: bold;">
</p>
</div>
</section>
<section id="how-people-use-chatgpt-chatterji-et.-al.-2025-3" class="slide level2">
<h2>How People Use ChatGPT (Chatterji et. al., 2025)</h2>
<div style="text-align: center; width: 100%; margin: auto;">
<img src="https://bcdanl.github.io/lec_figs/chatgpt-use-trend-prop.png" style="width: 100%; margin-bottom: -20px;">
<p style="font-weight: bold;">
</p>
</div>
</section>
<section id="how-people-use-chatgpt-chatterji-et.-al.-2025-4" class="slide level2">
<h2>How People Use ChatGPT (Chatterji et. al., 2025)</h2>
<ul>
<li class="fragment"><strong>Message Topics</strong>
<ul>
<li class="fragment">Top 3: <strong>Practical Guidance</strong>, <strong>Seeking Information</strong>, <strong>Writing</strong> ‚Üí ~<strong>80%</strong> of usage.<br>
</li>
<li class="fragment"><strong>Writing</strong> dominates work tasks ‚Üí shows ChatGPT‚Äôs unique edge over search engines.<br>
</li>
<li class="fragment"><strong>Programming</strong> and <strong>self-expression</strong> remain small shares.
<ul>
<li class="fragment"><strong>Self-expression</strong> = Greetings and Chitchat; Relationships and Personal Reflection; Games and Role Play</li>
</ul></li>
</ul></li>
</ul>
<div class="callout callout-note no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Note</strong></p>
</div>
<div class="callout-content">
<ul>
<li class="fragment">ChatGPT creates <strong>economic value through decision support</strong>.<br>
</li>
<li class="fragment">Especially valuable for <strong>knowledge-intensive jobs</strong>.<br>
</li>
<li class="fragment">Trend: more <strong>personal and creative use</strong> alongside work-related applications.<br>
</li>
</ul>
</div>
</div>
</div>
</section>
<section id="four-rules-for-co-intelligence-how-to-actually-work-with-ai-1" class="slide level2">
<h2>Four Rules for Co-Intelligence: How to actually work with AI</h2>
<ol type="1">
<li class="fragment">Always invite AI to the table</li>
<li class="fragment">Be the human in the loop (HITL)</li>
<li class="fragment">Treat AI like a person (but remember it isn‚Äôt)</li>
<li class="fragment">Assume this is the worst AI you‚Äôll ever use</li>
</ol>
</section>
<section id="principle-1-always-invite-ai-to-the-table" class="slide level2">
<h2>Principle #1 ‚Äî Always invite AI to the table</h2>
<ul>
<li class="fragment">Use AI for <strong>everything legal &amp; ethical</strong> to discover unexpected wins</li>
<li class="fragment"><strong>Try</strong>: critique an idea, draft memos, meeting notes, summaries, brainstorming</li>
<li class="fragment">Build a habit: keep a <strong>prompt log</strong> (what worked, what didn‚Äôt) and share with team</li>
</ul>
<!-- **Quick activity** -->
<!-- Pick a task this week. Ask AI for 3 alternatives + a 2-line rationale for each. Keep the best. -->
</section>
<section id="principle-2-be-the-human-in-the-loop-hitl" class="slide level2">
<h2>Principle #2 ‚Äî Be the human in the loop (HITL)</h2>
<div class="columns">
<div class="column" style="width:40%;">
<div style="text-align: center; width: 100%; margin: auto;">
<img src="https://bcdanl.github.io/lec_figs/fall-asleep-at-the-wheel.png" style="width: 100%; margin-bottom: -20px;">
<p style="font-weight: bold;">
</p>
</div>
</div><div class="column" style="width:60%;">
<ul>
<li class="fragment">Models can sound confident yet be <strong>wrong</strong> (hallucinations; math slips)</li>
<li class="fragment">People tend to <strong>‚Äúfall asleep at the wheel‚Äù</strong> when outputs look polished</li>
</ul>
</div>
</div>
<ul>
<li class="fragment"><strong>HITL checklist</strong>
<ul>
<li class="fragment">Require sources/quotes for factual claims<br>
</li>
<li class="fragment">Run a <strong>second pass</strong> (reword prompt or use a second model)<br>
</li>
<li class="fragment">For math/code, use other tools (calculator or IDE) and run <strong>tests</strong><br>
</li>
<li class="fragment">Ask: ‚ÄúWhat assumptions did you make? What could be wrong?‚Äù</li>
</ul></li>
</ul>
</section>
<section id="principle-3-treat-ai-like-a-person-but-remember-it-isnt" class="slide level2">
<h2>Principle #3 ‚Äî Treat AI like a person (but remember it isn‚Äôt)</h2>
<ul>
<li class="fragment"><strong>Useful design hack:</strong> give a <strong>role/persona + audience + constraints</strong><br>
</li>
<li class="fragment">Example:
<ul>
<li class="fragment">You are a TA helping intro microeconomics students.</li>
<li class="fragment">Constraints: concise and friendly tone, ‚â§200 words, prose paragraph with 1 example.</li>
<li class="fragment">Task: Explain utility maximization.</li>
<li class="fragment">Criteria: Must define the concept, connect to choice under constraints, and illustrate with a clear example.</li>
</ul></li>
<li class="fragment"><strong>Caution:</strong> it‚Äôs not sentient; it optimizes for <strong>plausibility</strong>, not truth</li>
</ul>
<!-- **Prompt pattern (copy/paste):**   -->
<!-- > You are a [role] helping [audience]. Constraints: [tone, length, format]. Task: [deliverable]. Criteria: [rubric]. First list assumptions; then produce the draft; end with 3 self-checks. -->
</section>
<section id="principle-4-assume-this-is-the-worst-ai-youll-ever-use" class="slide level2">
<h2>Principle #4 ‚Äî Assume this is the worst AI you‚Äôll ever use</h2>
<ul>
<li class="fragment">AI progress is <strong>rapid</strong>; today‚Äôs systems will likely be <strong>surpassed</strong> soon.</li>
<li class="fragment"><strong>Agent AI:</strong> LLMs that <strong>plan ‚Üí act ‚Üí learn</strong> with various external tools and memory, turning prompts into <strong>multi-step workflows</strong>
<ul>
<li class="fragment">e.g., A <strong>sales-order agent</strong> can validate orders, check inventory, generate invoices/shipping labels, and update ERP/CRM.</li>
</ul></li>
<li class="fragment">Treat the current moment as <strong>mid-journey</strong>, not the destination.<br>
</li>
<li class="fragment"><strong>Mindset shift</strong>: use today‚Äôs tools to <strong>learn</strong>, <strong>prototype</strong>, and <strong>prepare</strong> for better ones tomorrow.</li>
<li class="fragment">Those who <strong>keep up</strong> will adapt and <strong>thrive</strong> as capabilities improve.<br>
</li>
<li class="fragment">Focus on what you <strong>can control</strong>: <em>how</em> you use AI and <em>where</em> you apply it.</li>
<li class="fragment"><strong>Try it</strong>: Practice Co-Intelligence Rules ‚Üí <a href="https://bcdanl.github.io/danl-cw/danl-101-cw-03.html">Classwork 3</a>.</li>
</ul>
<!-- ## Why this mindset matters (for leaders) -->
<!-- ::: callout-quote -->
<!-- ‚ÄúWe don‚Äôt know how far it‚Äôs going to go‚Ä¶ We are in control of how we decide to use them and apply them.‚Äù -->
<!-- ‚Äî **Ethan Mollick** -->
<!-- ::: -->

<div class="footer footer-default">

</div>
</section></section>
    </div>
  </div>

  <script>window.backupDefine = window.define; window.define = undefined;</script>
  <script src="../site_libs/revealjs/dist/reveal.js"></script>
  <!-- reveal.js plugins -->
  <script src="../site_libs/revealjs/plugin/quarto-line-highlight/line-highlight.js"></script>
  <script src="../site_libs/revealjs/plugin/pdf-export/pdfexport.js"></script>
  <script src="../site_libs/revealjs/plugin/reveal-menu/menu.js"></script>
  <script src="../site_libs/revealjs/plugin/reveal-menu/quarto-menu.js"></script>
  <script src="../site_libs/revealjs/plugin/reveal-chalkboard/plugin.js"></script>
  <script src="../site_libs/revealjs/plugin/quarto-support/support.js"></script>
  

  <script src="../site_libs/revealjs/plugin/notes/notes.js"></script>
  <script src="../site_libs/revealjs/plugin/search/search.js"></script>
  <script src="../site_libs/revealjs/plugin/zoom/zoom.js"></script>
  <script src="../site_libs/revealjs/plugin/math/math.js"></script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script>

  <script>

      // Full list of configuration options available at:
      // https://revealjs.com/config/
      Reveal.initialize({
'controlsAuto': true,
'previewLinksAuto': false,
'smaller': false,
'pdfSeparateFragments': false,
'autoAnimateEasing': "ease",
'autoAnimateDuration': 1,
'autoAnimateUnmatched': true,
'menu': {"side":"left","useTextContentForMissingTitles":true,"markers":false,"loadIcons":false,"custom":[{"title":"Tools","icon":"<i class=\"fas fa-gear\"></i>","content":"<ul class=\"slide-menu-items\">\n<li class=\"slide-tool-item active\" data-item=\"0\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.fullscreen(event)\"><kbd>f</kbd> Fullscreen</a></li>\n<li class=\"slide-tool-item\" data-item=\"1\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.speakerMode(event)\"><kbd>s</kbd> Speaker View</a></li>\n<li class=\"slide-tool-item\" data-item=\"2\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.overview(event)\"><kbd>o</kbd> Slide Overview</a></li>\n<li class=\"slide-tool-item\" data-item=\"3\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.togglePdfExport(event)\"><kbd>e</kbd> PDF Export Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"4\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleChalkboard(event)\"><kbd>b</kbd> Toggle Chalkboard</a></li>\n<li class=\"slide-tool-item\" data-item=\"5\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleNotesCanvas(event)\"><kbd>c</kbd> Toggle Notes Canvas</a></li>\n<li class=\"slide-tool-item\" data-item=\"6\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.downloadDrawings(event)\"><kbd>d</kbd> Download Drawings</a></li>\n<li class=\"slide-tool-item\" data-item=\"7\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.keyboardHelp(event)\"><kbd>?</kbd> Keyboard Help</a></li>\n</ul>"}],"openButton":true},
'chalkboard': {"buttons":true},
'smaller': false,
 
        // Display controls in the bottom right corner
        controls: false,

        // Help the user learn the controls by providing hints, for example by
        // bouncing the down arrow when they first encounter a vertical slide
        controlsTutorial: false,

        // Determines where controls appear, "edges" or "bottom-right"
        controlsLayout: 'edges',

        // Visibility rule for backwards navigation arrows; "faded", "hidden"
        // or "visible"
        controlsBackArrows: 'faded',

        // Display a presentation progress bar
        progress: true,

        // Display the page number of the current slide
        slideNumber: 'c/t',

        // 'all', 'print', or 'speaker'
        showSlideNumber: 'all',

        // Add the current slide number to the URL hash so that reloading the
        // page/copying the URL will return you to the same slide
        hash: true,

        // Start with 1 for the hash rather than 0
        hashOneBasedIndex: false,

        // Flags if we should monitor the hash and change slides accordingly
        respondToHashChanges: true,

        // Push each slide change to the browser history
        history: true,

        // Enable keyboard shortcuts for navigation
        keyboard: true,

        // Enable the slide overview mode
        overview: true,

        // Disables the default reveal.js slide layout (scaling and centering)
        // so that you can use custom CSS layout
        disableLayout: false,

        // Vertical centering of slides
        center: false,

        // Enables touch navigation on devices with touch input
        touch: true,

        // Loop the presentation
        loop: false,

        // Change the presentation direction to be RTL
        rtl: false,

        // see https://revealjs.com/vertical-slides/#navigation-mode
        navigationMode: 'linear',

        // Randomizes the order of slides each time the presentation loads
        shuffle: false,

        // Turns fragments on and off globally
        fragments: true,

        // Flags whether to include the current fragment in the URL,
        // so that reloading brings you to the same fragment position
        fragmentInURL: false,

        // Flags if the presentation is running in an embedded mode,
        // i.e. contained within a limited portion of the screen
        embedded: false,

        // Flags if we should show a help overlay when the questionmark
        // key is pressed
        help: true,

        // Flags if it should be possible to pause the presentation (blackout)
        pause: true,

        // Flags if speaker notes should be visible to all viewers
        showNotes: false,

        // Global override for autoplaying embedded media (null/true/false)
        autoPlayMedia: null,

        // Global override for preloading lazy-loaded iframes (null/true/false)
        preloadIframes: null,

        // Number of milliseconds between automatically proceeding to the
        // next slide, disabled when set to 0, this value can be overwritten
        // by using a data-autoslide attribute on your slides
        autoSlide: 0,

        // Stop auto-sliding after user input
        autoSlideStoppable: true,

        // Use this method for navigation when auto-sliding
        autoSlideMethod: null,

        // Specify the average time in seconds that you think you will spend
        // presenting each slide. This is used to show a pacing timer in the
        // speaker view
        defaultTiming: null,

        // Enable slide navigation via mouse wheel
        mouseWheel: false,

        // The display mode that will be used to show slides
        display: 'block',

        // Hide cursor if inactive
        hideInactiveCursor: true,

        // Time before the cursor is hidden (in ms)
        hideCursorTime: 5000,

        // Opens links in an iframe preview overlay
        previewLinks: false,

        // Transition style (none/fade/slide/convex/concave/zoom)
        transition: 'none',

        // Transition speed (default/fast/slow)
        transitionSpeed: 'default',

        // Transition style for full page slide backgrounds
        // (none/fade/slide/convex/concave/zoom)
        backgroundTransition: 'none',

        // Number of slides away from the current that are visible
        viewDistance: 3,

        // Number of slides away from the current that are visible on mobile
        // devices. It is advisable to set this to a lower number than
        // viewDistance in order to save resources.
        mobileViewDistance: 2,

        // The "normal" size of the presentation, aspect ratio will be preserved
        // when the presentation is scaled to fit different resolutions. Can be
        // specified using percentage units.
        width: 1050,

        height: 700,

        // Factor of the display size that should remain empty around the content
        margin: 0.1,

        math: {
          mathjax: 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js',
          config: 'TeX-AMS_HTML-full',
          tex2jax: {
            inlineMath: [['\\(','\\)']],
            displayMath: [['\\[','\\]']],
            balanceBraces: true,
            processEscapes: false,
            processRefs: true,
            processEnvironments: true,
            preview: 'TeX',
            skipTags: ['script','noscript','style','textarea','pre','code'],
            ignoreClass: 'tex2jax_ignore',
            processClass: 'tex2jax_process'
          },
        },

        // reveal.js plugins
        plugins: [QuartoLineHighlight, PdfExport, RevealMenu, RevealChalkboard, QuartoSupport,

          RevealMath,
          RevealNotes,
          RevealSearch,
          RevealZoom
        ]
      });
    </script>
    <script>
    document.addEventListener('wheel', function(event) {
        if (event.deltaY > 0) {
            Reveal.next(); // Scroll down to go to the next slide
        } else {
            Reveal.prev(); // Scroll up to go to the previous slide
        }
    }, false);

    window.onload = function() {
        document.querySelectorAll('a').forEach(function(link) {
            link.setAttribute('target', '_blank');
        });
    };

    document.addEventListener('DOMContentLoaded', function() {
      // Query all anchor tags within code blocks (adjust the selector as needed)
      document.querySelectorAll('pre code a').forEach(function(element) {
        element.addEventListener('click', function(e) {
          e.preventDefault(); // Prevent the default anchor action
          e.stopPropagation(); // Stop the event from bubbling up
        });
      });
    });



    document.addEventListener('DOMContentLoaded', function() {
        // Target all span elements within code blocks that have IDs starting with 'cb'
        document.querySelectorAll('pre code span[id^="cb"]').forEach(function(element) {
            element.addEventListener('mouseenter', function() {
                // Apply yellow background color to the hovered span element
                this.style.backgroundColor = '#FFFF99';
            });
            element.addEventListener('mouseleave', function() {
                // Revert the background color when the mouse leaves the span element
                this.style.backgroundColor = '';
            });
        });
    });



    </script>
    
    <script>
      // htmlwidgets need to know to resize themselves when slides are shown/hidden.
      // Fire the "slideenter" event (handled by htmlwidgets.js) when the current
      // slide changes (different for each slide format).
      (function () {
        // dispatch for htmlwidgets
        function fireSlideEnter() {
          const event = window.document.createEvent("Event");
          event.initEvent("slideenter", true, true);
          window.document.dispatchEvent(event);
        }

        function fireSlideChanged(previousSlide, currentSlide) {
          fireSlideEnter();

          // dispatch for shiny
          if (window.jQuery) {
            if (previousSlide) {
              window.jQuery(previousSlide).trigger("hidden");
            }
            if (currentSlide) {
              window.jQuery(currentSlide).trigger("shown");
            }
          }
        }

        // hookup for slidy
        if (window.w3c_slidy) {
          window.w3c_slidy.add_observer(function (slide_num) {
            // slide_num starts at position 1
            fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);
          });
        }

      })();
    </script>

    <script id="quarto-html-after-body" type="application/javascript">
    window.document.addEventListener("DOMContentLoaded", function (event) {
      const toggleBodyColorMode = (bsSheetEl) => {
        const mode = bsSheetEl.getAttribute("data-mode");
        const bodyEl = window.document.querySelector("body");
        if (mode === "dark") {
          bodyEl.classList.add("quarto-dark");
          bodyEl.classList.remove("quarto-light");
        } else {
          bodyEl.classList.add("quarto-light");
          bodyEl.classList.remove("quarto-dark");
        }
      }
      const toggleBodyColorPrimary = () => {
        const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
        if (bsSheetEl) {
          toggleBodyColorMode(bsSheetEl);
        }
      }
      toggleBodyColorPrimary();  
      const tabsets =  window.document.querySelectorAll(".panel-tabset-tabby")
      tabsets.forEach(function(tabset) {
        const tabby = new Tabby('#' + tabset.id);
      });
      const isCodeAnnotation = (el) => {
        for (const clz of el.classList) {
          if (clz.startsWith('code-annotation-')) {                     
            return true;
          }
        }
        return false;
      }
      const clipboard = new window.ClipboardJS('.code-copy-button', {
        text: function(trigger) {
          const codeEl = trigger.previousElementSibling.cloneNode(true);
          for (const childEl of codeEl.children) {
            if (isCodeAnnotation(childEl)) {
              childEl.remove();
            }
          }
          return codeEl.innerText;
        }
      });
      clipboard.on('success', function(e) {
        // button target
        const button = e.trigger;
        // don't keep focus
        button.blur();
        // flash "checked"
        button.classList.add('code-copy-button-checked');
        var currentTitle = button.getAttribute("title");
        button.setAttribute("title", "Copied!");
        let tooltip;
        if (window.bootstrap) {
          button.setAttribute("data-bs-toggle", "tooltip");
          button.setAttribute("data-bs-placement", "left");
          button.setAttribute("data-bs-title", "Copied!");
          tooltip = new bootstrap.Tooltip(button, 
            { trigger: "manual", 
              customClass: "code-copy-button-tooltip",
              offset: [0, -8]});
          tooltip.show();    
        }
        setTimeout(function() {
          if (tooltip) {
            tooltip.hide();
            button.removeAttribute("data-bs-title");
            button.removeAttribute("data-bs-toggle");
            button.removeAttribute("data-bs-placement");
          }
          button.setAttribute("title", currentTitle);
          button.classList.remove('code-copy-button-checked');
        }, 1000);
        // clear code selection
        e.clearSelection();
      });
      function tippyHover(el, contentFn) {
        const config = {
          allowHTML: true,
          content: contentFn,
          maxWidth: 500,
          delay: 100,
          arrow: false,
          appendTo: function(el) {
              return el.closest('section.slide') || el.parentElement;
          },
          interactive: true,
          interactiveBorder: 10,
          theme: 'light-border',
          placement: 'bottom-start'
        };
          config['offset'] = [0,0];
          config['maxWidth'] = 700;
        window.tippy(el, config); 
      }
      const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
      for (var i=0; i<noterefs.length; i++) {
        const ref = noterefs[i];
        tippyHover(ref, function() {
          // use id or data attribute instead here
          let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
          try { href = new URL(href).hash; } catch {}
          const id = href.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          return note.innerHTML;
        });
      }
          let selectedAnnoteEl;
          const selectorForAnnotation = ( cell, annotation) => {
            let cellAttr = 'data-code-cell="' + cell + '"';
            let lineAttr = 'data-code-annotation="' +  annotation + '"';
            const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
            return selector;
          }
          const selectCodeLines = (annoteEl) => {
            const doc = window.document;
            const targetCell = annoteEl.getAttribute("data-target-cell");
            const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
            const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
            const lines = annoteSpan.getAttribute("data-code-lines").split(",");
            const lineIds = lines.map((line) => {
              return targetCell + "-" + line;
            })
            let top = null;
            let height = null;
            let parent = null;
            if (lineIds.length > 0) {
                //compute the position of the single el (top and bottom and make a div)
                const el = window.document.getElementById(lineIds[0]);
                top = el.offsetTop;
                height = el.offsetHeight;
                parent = el.parentElement.parentElement;
              if (lineIds.length > 1) {
                const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
                const bottom = lastEl.offsetTop + lastEl.offsetHeight;
                height = bottom - top;
              }
              if (top !== null && height !== null && parent !== null) {
                // cook up a div (if necessary) and position it 
                let div = window.document.getElementById("code-annotation-line-highlight");
                if (div === null) {
                  div = window.document.createElement("div");
                  div.setAttribute("id", "code-annotation-line-highlight");
                  div.style.position = 'absolute';
                  parent.appendChild(div);
                }
                div.style.top = top - 2 + "px";
                div.style.height = height + 4 + "px";
                let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
                if (gutterDiv === null) {
                  gutterDiv = window.document.createElement("div");
                  gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                  gutterDiv.style.position = 'absolute';
                  const codeCell = window.document.getElementById(targetCell);
                  const gutter = codeCell.querySelector('.code-annotation-gutter');
                  gutter.appendChild(gutterDiv);
                }
                gutterDiv.style.top = top - 2 + "px";
                gutterDiv.style.height = height + 4 + "px";
              }
              selectedAnnoteEl = annoteEl;
            }
          };
          const unselectCodeLines = () => {
            const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
            elementsIds.forEach((elId) => {
              const div = window.document.getElementById(elId);
              if (div) {
                div.remove();
              }
            });
            selectedAnnoteEl = undefined;
          };
          // Handle positioning of the toggle
          window.addEventListener(
          "resize",
          throttle(() => {
            elRect = undefined;
            if (selectedAnnoteEl) {
              selectCodeLines(selectedAnnoteEl);
            }
          }, 10)
          );
          function throttle(fn, ms) {
          let throttle = false;
          let timer;
            return (...args) => {
              if(!throttle) { // first call gets through
                  fn.apply(this, args);
                  throttle = true;
              } else { // all the others get throttled
                  if(timer) clearTimeout(timer); // cancel #2
                  timer = setTimeout(() => {
                    fn.apply(this, args);
                    timer = throttle = false;
                  }, ms);
              }
            };
          }
          const annoteTargets = window.document.querySelectorAll('.code-annotation-anchor');
          for (let i=0; i<annoteTargets.length; i++) {
            const annoteTarget = annoteTargets[i];
            const targetCell = annoteTarget.getAttribute("data-target-cell");
            const targetAnnotation = annoteTarget.getAttribute("data-target-annotation");
            const contentFn = () => {
              const content = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
              if (content) {
                const tipContent = content.cloneNode(true);
                tipContent.classList.add("code-annotation-tip-content");
                return tipContent.outerHTML;
              }
            }
            const config = {
              allowHTML: true,
              content: contentFn,
              onShow: (instance) => {
                selectCodeLines(instance.reference);
                instance.reference.classList.add('code-annotation-active');
                window.tippy.hideAll();
              },
              onHide: (instance) => {
                unselectCodeLines();
                instance.reference.classList.remove('code-annotation-active');
              },
              maxWidth: 300,
              delay: [50, 0],
              duration: [200, 0],
              offset: [5, 10],
              arrow: true,
              appendTo: function(el) {
                return el.parentElement.parentElement.parentElement;
              },
              interactive: true,
              interactiveBorder: 10,
              theme: 'light-border',
              placement: 'right',
              popperOptions: {
                modifiers: [
                {
                  name: 'flip',
                  options: {
                    flipVariations: false, // true by default
                    allowedAutoPlacements: ['right'],
                    fallbackPlacements: ['right', 'top', 'top-start', 'top-end'],
                  },
                },
                {
                  name: 'preventOverflow',
                  options: {
                    mainAxis: false,
                    altAxis: false
                  }
                }
                ]        
              }      
            };
            window.tippy(annoteTarget, config); 
          }
      const findCites = (el) => {
        const parentEl = el.parentElement;
        if (parentEl) {
          const cites = parentEl.dataset.cites;
          if (cites) {
            return {
              el,
              cites: cites.split(' ')
            };
          } else {
            return findCites(el.parentElement)
          }
        } else {
          return undefined;
        }
      };
      var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
      for (var i=0; i<bibliorefs.length; i++) {
        const ref = bibliorefs[i];
        const citeInfo = findCites(ref);
        if (citeInfo) {
          tippyHover(citeInfo.el, function() {
            var popup = window.document.createElement('div');
            citeInfo.cites.forEach(function(cite) {
              var citeDiv = window.document.createElement('div');
              citeDiv.classList.add('hanging-indent');
              citeDiv.classList.add('csl-entry');
              var biblioDiv = window.document.getElementById('ref-' + cite);
              if (biblioDiv) {
                citeDiv.innerHTML = biblioDiv.innerHTML;
              }
              popup.appendChild(citeDiv);
            });
            return popup.innerHTML;
          });
        }
      }
    });
    </script>
    

</body></html>